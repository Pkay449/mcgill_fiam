{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pickle\n",
    "from sklearn.ensemble import RandomForestClassifier, BaggingClassifier\n",
    "from sklearn.metrics import log_loss,accuracy_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. **MUST RUN 01-Data_Preprocessing/01-preprocessing_code.py AND 02-Feature_Engineering/01-fe_code.py BEFORE**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### We have the following dataset :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load objects/X_DATASET.pkl and objects/Y_DATASET.pkl\n",
    "X = pd.read_pickle('../objects/X_DATASET.pkl')\n",
    "Y = pd.read_pickle('../objects/Y_DATASET.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>age</th>\n",
       "      <th>aliq_at</th>\n",
       "      <th>aliq_mat</th>\n",
       "      <th>ami_126d</th>\n",
       "      <th>at_be</th>\n",
       "      <th>at_gr1</th>\n",
       "      <th>at_me</th>\n",
       "      <th>at_turnover</th>\n",
       "      <th>be_gr1a</th>\n",
       "      <th>be_me</th>\n",
       "      <th>...</th>\n",
       "      <th>tax_gr1a</th>\n",
       "      <th>turnover_126d</th>\n",
       "      <th>turnover_var_126d</th>\n",
       "      <th>z_score</th>\n",
       "      <th>zero_trades_126d</th>\n",
       "      <th>zero_trades_21d</th>\n",
       "      <th>zero_trades_252d</th>\n",
       "      <th>log_diff</th>\n",
       "      <th>frac_diff</th>\n",
       "      <th>sadf</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>132</td>\n",
       "      <td>0.985818</td>\n",
       "      <td>0.059472</td>\n",
       "      <td>0.000026</td>\n",
       "      <td>1.260918</td>\n",
       "      <td>0.651395</td>\n",
       "      <td>0.042021</td>\n",
       "      <td>1.028183</td>\n",
       "      <td>0.310450</td>\n",
       "      <td>0.033325</td>\n",
       "      <td>...</td>\n",
       "      <td>0.018174</td>\n",
       "      <td>0.003022</td>\n",
       "      <td>0.336775</td>\n",
       "      <td>43.032362</td>\n",
       "      <td>0.004157</td>\n",
       "      <td>0.005002</td>\n",
       "      <td>0.003226</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23522</th>\n",
       "      <td>600</td>\n",
       "      <td>0.610528</td>\n",
       "      <td>0.069305</td>\n",
       "      <td>0.000105</td>\n",
       "      <td>2.318300</td>\n",
       "      <td>-0.088342</td>\n",
       "      <td>0.149204</td>\n",
       "      <td>0.845524</td>\n",
       "      <td>-0.035351</td>\n",
       "      <td>0.064359</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.013003</td>\n",
       "      <td>0.002484</td>\n",
       "      <td>0.472298</td>\n",
       "      <td>10.113916</td>\n",
       "      <td>0.004944</td>\n",
       "      <td>0.006133</td>\n",
       "      <td>0.004822</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75824</th>\n",
       "      <td>325</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.004628</td>\n",
       "      <td>11.225380</td>\n",
       "      <td>0.119811</td>\n",
       "      <td>4.214064</td>\n",
       "      <td>0.086138</td>\n",
       "      <td>0.003968</td>\n",
       "      <td>0.375405</td>\n",
       "      <td>...</td>\n",
       "      <td>0.001342</td>\n",
       "      <td>0.003168</td>\n",
       "      <td>0.729098</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.003974</td>\n",
       "      <td>0.004473</td>\n",
       "      <td>0.004011</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>76014</th>\n",
       "      <td>60</td>\n",
       "      <td>0.616255</td>\n",
       "      <td>0.219590</td>\n",
       "      <td>0.000887</td>\n",
       "      <td>2.971011</td>\n",
       "      <td>0.093083</td>\n",
       "      <td>0.441731</td>\n",
       "      <td>1.506417</td>\n",
       "      <td>0.083045</td>\n",
       "      <td>0.148680</td>\n",
       "      <td>...</td>\n",
       "      <td>0.002257</td>\n",
       "      <td>0.003886</td>\n",
       "      <td>0.509687</td>\n",
       "      <td>5.348204</td>\n",
       "      <td>0.003278</td>\n",
       "      <td>0.004529</td>\n",
       "      <td>0.003185</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>129371</th>\n",
       "      <td>72</td>\n",
       "      <td>0.613501</td>\n",
       "      <td>0.109210</td>\n",
       "      <td>0.001278</td>\n",
       "      <td>2.574726</td>\n",
       "      <td>0.012174</td>\n",
       "      <td>0.169081</td>\n",
       "      <td>1.197614</td>\n",
       "      <td>0.235894</td>\n",
       "      <td>0.065669</td>\n",
       "      <td>...</td>\n",
       "      <td>0.029722</td>\n",
       "      <td>0.006013</td>\n",
       "      <td>1.056336</td>\n",
       "      <td>8.349836</td>\n",
       "      <td>0.001912</td>\n",
       "      <td>0.002674</td>\n",
       "      <td>0.002260</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>77686</th>\n",
       "      <td>654</td>\n",
       "      <td>0.348917</td>\n",
       "      <td>0.075573</td>\n",
       "      <td>0.000042</td>\n",
       "      <td>4.048223</td>\n",
       "      <td>0.026266</td>\n",
       "      <td>0.219966</td>\n",
       "      <td>0.381231</td>\n",
       "      <td>0.051937</td>\n",
       "      <td>0.054336</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.012389</td>\n",
       "      <td>0.003985</td>\n",
       "      <td>0.431750</td>\n",
       "      <td>5.694486</td>\n",
       "      <td>0.007268</td>\n",
       "      <td>0.005775</td>\n",
       "      <td>0.007261</td>\n",
       "      <td>0.171796</td>\n",
       "      <td>1.194481</td>\n",
       "      <td>-1.250080</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>110708</th>\n",
       "      <td>875</td>\n",
       "      <td>0.541670</td>\n",
       "      <td>0.469106</td>\n",
       "      <td>0.000079</td>\n",
       "      <td>3.390920</td>\n",
       "      <td>0.101955</td>\n",
       "      <td>2.579604</td>\n",
       "      <td>0.224324</td>\n",
       "      <td>0.020290</td>\n",
       "      <td>0.760739</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000223</td>\n",
       "      <td>0.012001</td>\n",
       "      <td>0.473572</td>\n",
       "      <td>0.648063</td>\n",
       "      <td>0.002548</td>\n",
       "      <td>0.001582</td>\n",
       "      <td>0.002992</td>\n",
       "      <td>0.018898</td>\n",
       "      <td>0.624563</td>\n",
       "      <td>-0.776705</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>77970</th>\n",
       "      <td>683</td>\n",
       "      <td>0.412350</td>\n",
       "      <td>0.106346</td>\n",
       "      <td>0.000052</td>\n",
       "      <td>23.971774</td>\n",
       "      <td>-0.034589</td>\n",
       "      <td>0.334292</td>\n",
       "      <td>1.221020</td>\n",
       "      <td>-0.062910</td>\n",
       "      <td>0.013945</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.009924</td>\n",
       "      <td>0.011881</td>\n",
       "      <td>0.590305</td>\n",
       "      <td>3.961255</td>\n",
       "      <td>0.002585</td>\n",
       "      <td>0.001735</td>\n",
       "      <td>0.003006</td>\n",
       "      <td>0.197150</td>\n",
       "      <td>1.228235</td>\n",
       "      <td>-1.518445</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9565</th>\n",
       "      <td>527</td>\n",
       "      <td>0.566360</td>\n",
       "      <td>0.404144</td>\n",
       "      <td>0.000205</td>\n",
       "      <td>5.153496</td>\n",
       "      <td>0.190384</td>\n",
       "      <td>1.926307</td>\n",
       "      <td>2.614588</td>\n",
       "      <td>-0.023169</td>\n",
       "      <td>0.373786</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.006986</td>\n",
       "      <td>0.013285</td>\n",
       "      <td>0.537003</td>\n",
       "      <td>3.973304</td>\n",
       "      <td>0.002241</td>\n",
       "      <td>0.002721</td>\n",
       "      <td>0.002129</td>\n",
       "      <td>0.039123</td>\n",
       "      <td>1.378815</td>\n",
       "      <td>-1.278062</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>131463</th>\n",
       "      <td>936</td>\n",
       "      <td>0.502752</td>\n",
       "      <td>0.448166</td>\n",
       "      <td>0.000064</td>\n",
       "      <td>2.224959</td>\n",
       "      <td>0.033296</td>\n",
       "      <td>1.989022</td>\n",
       "      <td>0.229889</td>\n",
       "      <td>-0.000261</td>\n",
       "      <td>0.893959</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000052</td>\n",
       "      <td>0.007469</td>\n",
       "      <td>0.391692</td>\n",
       "      <td>1.081265</td>\n",
       "      <td>0.004487</td>\n",
       "      <td>0.003086</td>\n",
       "      <td>0.004744</td>\n",
       "      <td>0.061175</td>\n",
       "      <td>1.207845</td>\n",
       "      <td>-2.129311</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>131464 rows × 150 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        age   aliq_at  aliq_mat  ami_126d      at_be    at_gr1     at_me  \\\n",
       "0       132  0.985818  0.059472  0.000026   1.260918  0.651395  0.042021   \n",
       "23522   600  0.610528  0.069305  0.000105   2.318300 -0.088342  0.149204   \n",
       "75824   325       NaN       NaN  0.004628  11.225380  0.119811  4.214064   \n",
       "76014    60  0.616255  0.219590  0.000887   2.971011  0.093083  0.441731   \n",
       "129371   72  0.613501  0.109210  0.001278   2.574726  0.012174  0.169081   \n",
       "...     ...       ...       ...       ...        ...       ...       ...   \n",
       "77686   654  0.348917  0.075573  0.000042   4.048223  0.026266  0.219966   \n",
       "110708  875  0.541670  0.469106  0.000079   3.390920  0.101955  2.579604   \n",
       "77970   683  0.412350  0.106346  0.000052  23.971774 -0.034589  0.334292   \n",
       "9565    527  0.566360  0.404144  0.000205   5.153496  0.190384  1.926307   \n",
       "131463  936  0.502752  0.448166  0.000064   2.224959  0.033296  1.989022   \n",
       "\n",
       "        at_turnover   be_gr1a     be_me  ...  tax_gr1a  turnover_126d  \\\n",
       "0          1.028183  0.310450  0.033325  ...  0.018174       0.003022   \n",
       "23522      0.845524 -0.035351  0.064359  ... -0.013003       0.002484   \n",
       "75824      0.086138  0.003968  0.375405  ...  0.001342       0.003168   \n",
       "76014      1.506417  0.083045  0.148680  ...  0.002257       0.003886   \n",
       "129371     1.197614  0.235894  0.065669  ...  0.029722       0.006013   \n",
       "...             ...       ...       ...  ...       ...            ...   \n",
       "77686      0.381231  0.051937  0.054336  ... -0.012389       0.003985   \n",
       "110708     0.224324  0.020290  0.760739  ...  0.000223       0.012001   \n",
       "77970      1.221020 -0.062910  0.013945  ... -0.009924       0.011881   \n",
       "9565       2.614588 -0.023169  0.373786  ... -0.006986       0.013285   \n",
       "131463     0.229889 -0.000261  0.893959  ...  0.000052       0.007469   \n",
       "\n",
       "        turnover_var_126d    z_score  zero_trades_126d  zero_trades_21d  \\\n",
       "0                0.336775  43.032362          0.004157         0.005002   \n",
       "23522            0.472298  10.113916          0.004944         0.006133   \n",
       "75824            0.729098        NaN          0.003974         0.004473   \n",
       "76014            0.509687   5.348204          0.003278         0.004529   \n",
       "129371           1.056336   8.349836          0.001912         0.002674   \n",
       "...                   ...        ...               ...              ...   \n",
       "77686            0.431750   5.694486          0.007268         0.005775   \n",
       "110708           0.473572   0.648063          0.002548         0.001582   \n",
       "77970            0.590305   3.961255          0.002585         0.001735   \n",
       "9565             0.537003   3.973304          0.002241         0.002721   \n",
       "131463           0.391692   1.081265          0.004487         0.003086   \n",
       "\n",
       "        zero_trades_252d  log_diff  frac_diff      sadf  \n",
       "0               0.003226  0.000000   0.000000  0.000000  \n",
       "23522           0.004822  0.000000   0.000000  0.000000  \n",
       "75824           0.004011  0.000000   0.000000  0.000000  \n",
       "76014           0.003185  0.000000   0.000000  0.000000  \n",
       "129371          0.002260  0.000000   0.000000  0.000000  \n",
       "...                  ...       ...        ...       ...  \n",
       "77686           0.007261  0.171796   1.194481 -1.250080  \n",
       "110708          0.002992  0.018898   0.624563 -0.776705  \n",
       "77970           0.003006  0.197150   1.228235 -1.518445  \n",
       "9565            0.002129  0.039123   1.378815 -1.278062  \n",
       "131463          0.004744  0.061175   1.207845 -2.129311  \n",
       "\n",
       "[131464 rows x 150 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>stock_exret</th>\n",
       "      <th>target</th>\n",
       "      <th>prediction</th>\n",
       "      <th>probability</th>\n",
       "      <th>t1</th>\n",
       "      <th>t1_index</th>\n",
       "      <th>weight_attr</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.018070</td>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.364712</td>\n",
       "      <td>2000-01-31</td>\n",
       "      <td>2000-01-03</td>\n",
       "      <td>0.018070</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23522</th>\n",
       "      <td>0.001539</td>\n",
       "      <td>1</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>0.144033</td>\n",
       "      <td>2000-01-31</td>\n",
       "      <td>2000-01-03</td>\n",
       "      <td>0.001539</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75824</th>\n",
       "      <td>0.054724</td>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.405740</td>\n",
       "      <td>2000-01-31</td>\n",
       "      <td>2000-01-03</td>\n",
       "      <td>0.054724</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>76014</th>\n",
       "      <td>0.009531</td>\n",
       "      <td>1</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>0.533961</td>\n",
       "      <td>2000-01-31</td>\n",
       "      <td>2000-01-03</td>\n",
       "      <td>0.009531</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>129371</th>\n",
       "      <td>0.389768</td>\n",
       "      <td>1</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>0.157432</td>\n",
       "      <td>2000-01-31</td>\n",
       "      <td>2000-01-03</td>\n",
       "      <td>0.389768</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>77686</th>\n",
       "      <td>0.065845</td>\n",
       "      <td>1</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>0.661575</td>\n",
       "      <td>2023-12-29</td>\n",
       "      <td>2023-12-01</td>\n",
       "      <td>0.065845</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>110708</th>\n",
       "      <td>0.031191</td>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.186675</td>\n",
       "      <td>2023-12-29</td>\n",
       "      <td>2023-12-01</td>\n",
       "      <td>0.031191</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>77970</th>\n",
       "      <td>-0.009602</td>\n",
       "      <td>-1</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.822283</td>\n",
       "      <td>2023-12-29</td>\n",
       "      <td>2023-12-01</td>\n",
       "      <td>0.009602</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9565</th>\n",
       "      <td>0.105924</td>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.973849</td>\n",
       "      <td>2023-12-29</td>\n",
       "      <td>2023-12-01</td>\n",
       "      <td>0.105924</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>131463</th>\n",
       "      <td>0.042407</td>\n",
       "      <td>1</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>0.362189</td>\n",
       "      <td>2023-12-29</td>\n",
       "      <td>2023-12-01</td>\n",
       "      <td>0.042407</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>131464 rows × 7 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        stock_exret  target  prediction  probability          t1    t1_index  \\\n",
       "0          0.018070       1         1.0     0.364712  2000-01-31  2000-01-03   \n",
       "23522      0.001539       1        -1.0     0.144033  2000-01-31  2000-01-03   \n",
       "75824      0.054724       1         1.0     0.405740  2000-01-31  2000-01-03   \n",
       "76014      0.009531       1        -1.0     0.533961  2000-01-31  2000-01-03   \n",
       "129371     0.389768       1        -1.0     0.157432  2000-01-31  2000-01-03   \n",
       "...             ...     ...         ...          ...         ...         ...   \n",
       "77686      0.065845       1        -1.0     0.661575  2023-12-29  2023-12-01   \n",
       "110708     0.031191       1         1.0     0.186675  2023-12-29  2023-12-01   \n",
       "77970     -0.009602      -1         1.0     0.822283  2023-12-29  2023-12-01   \n",
       "9565       0.105924       1         1.0     0.973849  2023-12-29  2023-12-01   \n",
       "131463     0.042407       1        -1.0     0.362189  2023-12-29  2023-12-01   \n",
       "\n",
       "        weight_attr  \n",
       "0          0.018070  \n",
       "23522      0.001539  \n",
       "75824      0.054724  \n",
       "76014      0.009531  \n",
       "129371     0.389768  \n",
       "...             ...  \n",
       "77686      0.065845  \n",
       "110708     0.031191  \n",
       "77970      0.009602  \n",
       "9565       0.105924  \n",
       "131463     0.042407  \n",
       "\n",
       "[131464 rows x 7 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th>age</th>\n",
       "      <th>aliq_at</th>\n",
       "      <th>aliq_mat</th>\n",
       "      <th>ami_126d</th>\n",
       "      <th>at_be</th>\n",
       "      <th>at_gr1</th>\n",
       "      <th>at_me</th>\n",
       "      <th>at_turnover</th>\n",
       "      <th>be_gr1a</th>\n",
       "      <th>be_me</th>\n",
       "      <th>...</th>\n",
       "      <th>tax_gr1a</th>\n",
       "      <th>turnover_126d</th>\n",
       "      <th>turnover_var_126d</th>\n",
       "      <th>z_score</th>\n",
       "      <th>zero_trades_126d</th>\n",
       "      <th>zero_trades_21d</th>\n",
       "      <th>zero_trades_252d</th>\n",
       "      <th>log_diff</th>\n",
       "      <th>frac_diff</th>\n",
       "      <th>sadf</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>t1_index</th>\n",
       "      <th>index</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th rowspan=\"5\" valign=\"top\">2000-01-03</th>\n",
       "      <th>0</th>\n",
       "      <td>132</td>\n",
       "      <td>0.985818</td>\n",
       "      <td>0.059472</td>\n",
       "      <td>0.000026</td>\n",
       "      <td>1.260918</td>\n",
       "      <td>0.651395</td>\n",
       "      <td>0.042021</td>\n",
       "      <td>1.028183</td>\n",
       "      <td>0.310450</td>\n",
       "      <td>0.033325</td>\n",
       "      <td>...</td>\n",
       "      <td>0.018174</td>\n",
       "      <td>0.003022</td>\n",
       "      <td>0.336775</td>\n",
       "      <td>43.032362</td>\n",
       "      <td>0.004157</td>\n",
       "      <td>0.005002</td>\n",
       "      <td>0.003226</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23522</th>\n",
       "      <td>600</td>\n",
       "      <td>0.610528</td>\n",
       "      <td>0.069305</td>\n",
       "      <td>0.000105</td>\n",
       "      <td>2.318300</td>\n",
       "      <td>-0.088342</td>\n",
       "      <td>0.149204</td>\n",
       "      <td>0.845524</td>\n",
       "      <td>-0.035351</td>\n",
       "      <td>0.064359</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.013003</td>\n",
       "      <td>0.002484</td>\n",
       "      <td>0.472298</td>\n",
       "      <td>10.113916</td>\n",
       "      <td>0.004944</td>\n",
       "      <td>0.006133</td>\n",
       "      <td>0.004822</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75824</th>\n",
       "      <td>325</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.004628</td>\n",
       "      <td>11.225380</td>\n",
       "      <td>0.119811</td>\n",
       "      <td>4.214064</td>\n",
       "      <td>0.086138</td>\n",
       "      <td>0.003968</td>\n",
       "      <td>0.375405</td>\n",
       "      <td>...</td>\n",
       "      <td>0.001342</td>\n",
       "      <td>0.003168</td>\n",
       "      <td>0.729098</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.003974</td>\n",
       "      <td>0.004473</td>\n",
       "      <td>0.004011</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>76014</th>\n",
       "      <td>60</td>\n",
       "      <td>0.616255</td>\n",
       "      <td>0.219590</td>\n",
       "      <td>0.000887</td>\n",
       "      <td>2.971011</td>\n",
       "      <td>0.093083</td>\n",
       "      <td>0.441731</td>\n",
       "      <td>1.506417</td>\n",
       "      <td>0.083045</td>\n",
       "      <td>0.148680</td>\n",
       "      <td>...</td>\n",
       "      <td>0.002257</td>\n",
       "      <td>0.003886</td>\n",
       "      <td>0.509687</td>\n",
       "      <td>5.348204</td>\n",
       "      <td>0.003278</td>\n",
       "      <td>0.004529</td>\n",
       "      <td>0.003185</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>129371</th>\n",
       "      <td>72</td>\n",
       "      <td>0.613501</td>\n",
       "      <td>0.109210</td>\n",
       "      <td>0.001278</td>\n",
       "      <td>2.574726</td>\n",
       "      <td>0.012174</td>\n",
       "      <td>0.169081</td>\n",
       "      <td>1.197614</td>\n",
       "      <td>0.235894</td>\n",
       "      <td>0.065669</td>\n",
       "      <td>...</td>\n",
       "      <td>0.029722</td>\n",
       "      <td>0.006013</td>\n",
       "      <td>1.056336</td>\n",
       "      <td>8.349836</td>\n",
       "      <td>0.001912</td>\n",
       "      <td>0.002674</td>\n",
       "      <td>0.002260</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"5\" valign=\"top\">2023-12-01</th>\n",
       "      <th>77686</th>\n",
       "      <td>654</td>\n",
       "      <td>0.348917</td>\n",
       "      <td>0.075573</td>\n",
       "      <td>0.000042</td>\n",
       "      <td>4.048223</td>\n",
       "      <td>0.026266</td>\n",
       "      <td>0.219966</td>\n",
       "      <td>0.381231</td>\n",
       "      <td>0.051937</td>\n",
       "      <td>0.054336</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.012389</td>\n",
       "      <td>0.003985</td>\n",
       "      <td>0.431750</td>\n",
       "      <td>5.694486</td>\n",
       "      <td>0.007268</td>\n",
       "      <td>0.005775</td>\n",
       "      <td>0.007261</td>\n",
       "      <td>0.171796</td>\n",
       "      <td>1.194481</td>\n",
       "      <td>-1.250080</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>110708</th>\n",
       "      <td>875</td>\n",
       "      <td>0.541670</td>\n",
       "      <td>0.469106</td>\n",
       "      <td>0.000079</td>\n",
       "      <td>3.390920</td>\n",
       "      <td>0.101955</td>\n",
       "      <td>2.579604</td>\n",
       "      <td>0.224324</td>\n",
       "      <td>0.020290</td>\n",
       "      <td>0.760739</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000223</td>\n",
       "      <td>0.012001</td>\n",
       "      <td>0.473572</td>\n",
       "      <td>0.648063</td>\n",
       "      <td>0.002548</td>\n",
       "      <td>0.001582</td>\n",
       "      <td>0.002992</td>\n",
       "      <td>0.018898</td>\n",
       "      <td>0.624563</td>\n",
       "      <td>-0.776705</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>77970</th>\n",
       "      <td>683</td>\n",
       "      <td>0.412350</td>\n",
       "      <td>0.106346</td>\n",
       "      <td>0.000052</td>\n",
       "      <td>23.971774</td>\n",
       "      <td>-0.034589</td>\n",
       "      <td>0.334292</td>\n",
       "      <td>1.221020</td>\n",
       "      <td>-0.062910</td>\n",
       "      <td>0.013945</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.009924</td>\n",
       "      <td>0.011881</td>\n",
       "      <td>0.590305</td>\n",
       "      <td>3.961255</td>\n",
       "      <td>0.002585</td>\n",
       "      <td>0.001735</td>\n",
       "      <td>0.003006</td>\n",
       "      <td>0.197150</td>\n",
       "      <td>1.228235</td>\n",
       "      <td>-1.518445</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9565</th>\n",
       "      <td>527</td>\n",
       "      <td>0.566360</td>\n",
       "      <td>0.404144</td>\n",
       "      <td>0.000205</td>\n",
       "      <td>5.153496</td>\n",
       "      <td>0.190384</td>\n",
       "      <td>1.926307</td>\n",
       "      <td>2.614588</td>\n",
       "      <td>-0.023169</td>\n",
       "      <td>0.373786</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.006986</td>\n",
       "      <td>0.013285</td>\n",
       "      <td>0.537003</td>\n",
       "      <td>3.973304</td>\n",
       "      <td>0.002241</td>\n",
       "      <td>0.002721</td>\n",
       "      <td>0.002129</td>\n",
       "      <td>0.039123</td>\n",
       "      <td>1.378815</td>\n",
       "      <td>-1.278062</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>131463</th>\n",
       "      <td>936</td>\n",
       "      <td>0.502752</td>\n",
       "      <td>0.448166</td>\n",
       "      <td>0.000064</td>\n",
       "      <td>2.224959</td>\n",
       "      <td>0.033296</td>\n",
       "      <td>1.989022</td>\n",
       "      <td>0.229889</td>\n",
       "      <td>-0.000261</td>\n",
       "      <td>0.893959</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000052</td>\n",
       "      <td>0.007469</td>\n",
       "      <td>0.391692</td>\n",
       "      <td>1.081265</td>\n",
       "      <td>0.004487</td>\n",
       "      <td>0.003086</td>\n",
       "      <td>0.004744</td>\n",
       "      <td>0.061175</td>\n",
       "      <td>1.207845</td>\n",
       "      <td>-2.129311</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>131464 rows × 150 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                   age   aliq_at  aliq_mat  ami_126d      at_be    at_gr1  \\\n",
       "t1_index   index                                                            \n",
       "2000-01-03 0       132  0.985818  0.059472  0.000026   1.260918  0.651395   \n",
       "           23522   600  0.610528  0.069305  0.000105   2.318300 -0.088342   \n",
       "           75824   325       NaN       NaN  0.004628  11.225380  0.119811   \n",
       "           76014    60  0.616255  0.219590  0.000887   2.971011  0.093083   \n",
       "           129371   72  0.613501  0.109210  0.001278   2.574726  0.012174   \n",
       "...                ...       ...       ...       ...        ...       ...   \n",
       "2023-12-01 77686   654  0.348917  0.075573  0.000042   4.048223  0.026266   \n",
       "           110708  875  0.541670  0.469106  0.000079   3.390920  0.101955   \n",
       "           77970   683  0.412350  0.106346  0.000052  23.971774 -0.034589   \n",
       "           9565    527  0.566360  0.404144  0.000205   5.153496  0.190384   \n",
       "           131463  936  0.502752  0.448166  0.000064   2.224959  0.033296   \n",
       "\n",
       "                      at_me  at_turnover   be_gr1a     be_me  ...  tax_gr1a  \\\n",
       "t1_index   index                                              ...             \n",
       "2000-01-03 0       0.042021     1.028183  0.310450  0.033325  ...  0.018174   \n",
       "           23522   0.149204     0.845524 -0.035351  0.064359  ... -0.013003   \n",
       "           75824   4.214064     0.086138  0.003968  0.375405  ...  0.001342   \n",
       "           76014   0.441731     1.506417  0.083045  0.148680  ...  0.002257   \n",
       "           129371  0.169081     1.197614  0.235894  0.065669  ...  0.029722   \n",
       "...                     ...          ...       ...       ...  ...       ...   \n",
       "2023-12-01 77686   0.219966     0.381231  0.051937  0.054336  ... -0.012389   \n",
       "           110708  2.579604     0.224324  0.020290  0.760739  ...  0.000223   \n",
       "           77970   0.334292     1.221020 -0.062910  0.013945  ... -0.009924   \n",
       "           9565    1.926307     2.614588 -0.023169  0.373786  ... -0.006986   \n",
       "           131463  1.989022     0.229889 -0.000261  0.893959  ...  0.000052   \n",
       "\n",
       "                   turnover_126d  turnover_var_126d    z_score  \\\n",
       "t1_index   index                                                 \n",
       "2000-01-03 0            0.003022           0.336775  43.032362   \n",
       "           23522        0.002484           0.472298  10.113916   \n",
       "           75824        0.003168           0.729098        NaN   \n",
       "           76014        0.003886           0.509687   5.348204   \n",
       "           129371       0.006013           1.056336   8.349836   \n",
       "...                          ...                ...        ...   \n",
       "2023-12-01 77686        0.003985           0.431750   5.694486   \n",
       "           110708       0.012001           0.473572   0.648063   \n",
       "           77970        0.011881           0.590305   3.961255   \n",
       "           9565         0.013285           0.537003   3.973304   \n",
       "           131463       0.007469           0.391692   1.081265   \n",
       "\n",
       "                   zero_trades_126d  zero_trades_21d  zero_trades_252d  \\\n",
       "t1_index   index                                                         \n",
       "2000-01-03 0               0.004157         0.005002          0.003226   \n",
       "           23522           0.004944         0.006133          0.004822   \n",
       "           75824           0.003974         0.004473          0.004011   \n",
       "           76014           0.003278         0.004529          0.003185   \n",
       "           129371          0.001912         0.002674          0.002260   \n",
       "...                             ...              ...               ...   \n",
       "2023-12-01 77686           0.007268         0.005775          0.007261   \n",
       "           110708          0.002548         0.001582          0.002992   \n",
       "           77970           0.002585         0.001735          0.003006   \n",
       "           9565            0.002241         0.002721          0.002129   \n",
       "           131463          0.004487         0.003086          0.004744   \n",
       "\n",
       "                   log_diff  frac_diff      sadf  \n",
       "t1_index   index                                  \n",
       "2000-01-03 0       0.000000   0.000000  0.000000  \n",
       "           23522   0.000000   0.000000  0.000000  \n",
       "           75824   0.000000   0.000000  0.000000  \n",
       "           76014   0.000000   0.000000  0.000000  \n",
       "           129371  0.000000   0.000000  0.000000  \n",
       "...                     ...        ...       ...  \n",
       "2023-12-01 77686   0.171796   1.194481 -1.250080  \n",
       "           110708  0.018898   0.624563 -0.776705  \n",
       "           77970   0.197150   1.228235 -1.518445  \n",
       "           9565    0.039123   1.378815 -1.278062  \n",
       "           131463  0.061175   1.207845 -2.129311  \n",
       "\n",
       "[131464 rows x 150 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X['t1_index'] = Y['t1_index']\n",
    "X.reset_index(inplace=True)\n",
    "X.set_index(['t1_index', 'index'], inplace=True)\n",
    "X"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### X contains the feautes given and ADDED BY US"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th>stock_exret</th>\n",
       "      <th>target</th>\n",
       "      <th>prediction</th>\n",
       "      <th>probability</th>\n",
       "      <th>t1</th>\n",
       "      <th>weight_attr</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>t1_index</th>\n",
       "      <th>index</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th rowspan=\"5\" valign=\"top\">2000-01-03</th>\n",
       "      <th>0</th>\n",
       "      <td>0.018070</td>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.364712</td>\n",
       "      <td>2000-01-31</td>\n",
       "      <td>0.018070</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23522</th>\n",
       "      <td>0.001539</td>\n",
       "      <td>1</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>0.144033</td>\n",
       "      <td>2000-01-31</td>\n",
       "      <td>0.001539</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75824</th>\n",
       "      <td>0.054724</td>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.405740</td>\n",
       "      <td>2000-01-31</td>\n",
       "      <td>0.054724</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>76014</th>\n",
       "      <td>0.009531</td>\n",
       "      <td>1</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>0.533961</td>\n",
       "      <td>2000-01-31</td>\n",
       "      <td>0.009531</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>129371</th>\n",
       "      <td>0.389768</td>\n",
       "      <td>1</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>0.157432</td>\n",
       "      <td>2000-01-31</td>\n",
       "      <td>0.389768</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"5\" valign=\"top\">2023-12-01</th>\n",
       "      <th>77686</th>\n",
       "      <td>0.065845</td>\n",
       "      <td>1</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>0.661575</td>\n",
       "      <td>2023-12-29</td>\n",
       "      <td>0.065845</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>110708</th>\n",
       "      <td>0.031191</td>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.186675</td>\n",
       "      <td>2023-12-29</td>\n",
       "      <td>0.031191</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>77970</th>\n",
       "      <td>-0.009602</td>\n",
       "      <td>-1</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.822283</td>\n",
       "      <td>2023-12-29</td>\n",
       "      <td>0.009602</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9565</th>\n",
       "      <td>0.105924</td>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.973849</td>\n",
       "      <td>2023-12-29</td>\n",
       "      <td>0.105924</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>131463</th>\n",
       "      <td>0.042407</td>\n",
       "      <td>1</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>0.362189</td>\n",
       "      <td>2023-12-29</td>\n",
       "      <td>0.042407</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>131464 rows × 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                   stock_exret  target  prediction  probability          t1  \\\n",
       "t1_index   index                                                              \n",
       "2000-01-03 0          0.018070       1         1.0     0.364712  2000-01-31   \n",
       "           23522      0.001539       1        -1.0     0.144033  2000-01-31   \n",
       "           75824      0.054724       1         1.0     0.405740  2000-01-31   \n",
       "           76014      0.009531       1        -1.0     0.533961  2000-01-31   \n",
       "           129371     0.389768       1        -1.0     0.157432  2000-01-31   \n",
       "...                        ...     ...         ...          ...         ...   \n",
       "2023-12-01 77686      0.065845       1        -1.0     0.661575  2023-12-29   \n",
       "           110708     0.031191       1         1.0     0.186675  2023-12-29   \n",
       "           77970     -0.009602      -1         1.0     0.822283  2023-12-29   \n",
       "           9565       0.105924       1         1.0     0.973849  2023-12-29   \n",
       "           131463     0.042407       1        -1.0     0.362189  2023-12-29   \n",
       "\n",
       "                   weight_attr  \n",
       "t1_index   index                \n",
       "2000-01-03 0          0.018070  \n",
       "           23522      0.001539  \n",
       "           75824      0.054724  \n",
       "           76014      0.009531  \n",
       "           129371     0.389768  \n",
       "...                        ...  \n",
       "2023-12-01 77686      0.065845  \n",
       "           110708     0.031191  \n",
       "           77970      0.009602  \n",
       "           9565       0.105924  \n",
       "           131463     0.042407  \n",
       "\n",
       "[131464 rows x 6 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Y = Y.reset_index()\n",
    "Y.set_index(['t1_index', 'index'], inplace=True)\n",
    "Y\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Currently, in $Y$,\n",
    "\n",
    "*prediction* and *probability* are randomly generated, those are the columns\n",
    "we must fill for the investment perdiod (2008-) we our true probabilities and predictions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## WE WANT TO PREDICT TARGET, which is simply the sign of stock_exret\n",
    "\n",
    "## Weight_attr IS SO IMPORANT, PLEASE DON'T IGNORE \n",
    "\n",
    "As Uday did in feature importance, this weight_attr must be rescaled for the \n",
    "training period. Right now, it is scaled for the whole dataset, you must reajust\n",
    "it for the training period such that the suym of the weight = Size of the training period\n",
    "\n",
    "*= X_train.shape[0]/X_train['weight_attr'].sum()\n",
    "\n",
    "Uday did it I believe\n",
    "\n",
    "Refer to section 4.6 if you have any doubts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hyperparameter Tuning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Method\n",
    "\n",
    "The method is explained in the FIAM instructions, please refer to pages\\\n",
    "6 and 7, sections **Performance evaluation** AND **Training Procedures**\\\n",
    "(especially training procedures)\n",
    "\n",
    "As they mention it in the doc, they provided an implementation of the method,\n",
    "\n",
    "penalized_linear_hackathon.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Start: 2000-01-01 00:00:00, Train End: 2008-01-01 00:00:00, Val Start: 2008-01-01 00:00:00, Val End: 2010-01-01 00:00:00, Test Start: 2010-01-01 00:00:00, Test End: 2011-01-01 00:00:00\n",
      "Fitting 1 folds for each of 1 candidates, totalling 1 fits\n",
      "[CV] END estimator__max_depth=43, estimator__max_features=log2, estimator__min_samples_leaf=1, estimator__min_samples_split=8, estimator__n_estimators=116, max_features=0.8796910002727693, max_samples=0.696850157946487, n_estimators=92; total time= 3.4min\n",
      "Log Loss on Test Set: -0.6810979514752862\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Python(78664) MallocStackLogging: can't turn off malloc stack logging because it was not enabled.\n",
      "Python(78693) MallocStackLogging: can't turn off malloc stack logging because it was not enabled.\n",
      "Python(78701) MallocStackLogging: can't turn off malloc stack logging because it was not enabled.\n",
      "Python(78710) MallocStackLogging: can't turn off malloc stack logging because it was not enabled.\n",
      "Python(78711) MallocStackLogging: can't turn off malloc stack logging because it was not enabled.\n",
      "Python(78712) MallocStackLogging: can't turn off malloc stack logging because it was not enabled.\n",
      "/Users/moosasikkander/Library/Python/3.9/lib/python/site-packages/joblib/externals/loky/process_executor.py:752: UserWarning: A worker stopped while some jobs were given to the executor. This can be caused by a too short worker timeout or by a memory leak.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[15], line 135\u001b[0m\n\u001b[1;32m    132\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mLog Loss on Test Set:\u001b[39m\u001b[38;5;124m\"\u001b[39m, score_)\n\u001b[1;32m    134\u001b[0m \u001b[38;5;66;03m# Store predictions in Y_test\u001b[39;00m\n\u001b[0;32m--> 135\u001b[0m Y_test[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mprediction\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[43mbest_estimator\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpredict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_test_vals\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    136\u001b[0m Y_test[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mprobability\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m prob\u001b[38;5;241m.\u001b[39mmax(axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m    138\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPredictions on Test Set:\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/sklearn/ensemble/_bagging.py:807\u001b[0m, in \u001b[0;36mBaggingClassifier.predict\u001b[0;34m(self, X)\u001b[0m\n\u001b[1;32m    789\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mpredict\u001b[39m(\u001b[38;5;28mself\u001b[39m, X):\n\u001b[1;32m    790\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Predict class for X.\u001b[39;00m\n\u001b[1;32m    791\u001b[0m \n\u001b[1;32m    792\u001b[0m \u001b[38;5;124;03m    The predicted class of an input sample is computed as the class with\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    805\u001b[0m \u001b[38;5;124;03m        The predicted classes.\u001b[39;00m\n\u001b[1;32m    806\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 807\u001b[0m     predicted_probabilitiy \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpredict_proba\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    808\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mclasses_\u001b[38;5;241m.\u001b[39mtake((np\u001b[38;5;241m.\u001b[39margmax(predicted_probabilitiy, axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)), axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m)\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/sklearn/ensemble/_bagging.py:845\u001b[0m, in \u001b[0;36mBaggingClassifier.predict_proba\u001b[0;34m(self, X)\u001b[0m\n\u001b[1;32m    842\u001b[0m \u001b[38;5;66;03m# Parallel loop\u001b[39;00m\n\u001b[1;32m    843\u001b[0m n_jobs, _, starts \u001b[38;5;241m=\u001b[39m _partition_estimators(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_estimators, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_jobs)\n\u001b[0;32m--> 845\u001b[0m all_proba \u001b[38;5;241m=\u001b[39m \u001b[43mParallel\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    846\u001b[0m \u001b[43m    \u001b[49m\u001b[43mn_jobs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mn_jobs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mverbose\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_parallel_args\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    847\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    848\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdelayed\u001b[49m\u001b[43m(\u001b[49m\u001b[43m_parallel_predict_proba\u001b[49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    849\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mestimators_\u001b[49m\u001b[43m[\u001b[49m\u001b[43mstarts\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mstarts\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    850\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mestimators_features_\u001b[49m\u001b[43m[\u001b[49m\u001b[43mstarts\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mstarts\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    851\u001b[0m \u001b[43m        \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    852\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mn_classes_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    853\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    854\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mi\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mrange\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mn_jobs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    855\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    857\u001b[0m \u001b[38;5;66;03m# Reduce\u001b[39;00m\n\u001b[1;32m    858\u001b[0m proba \u001b[38;5;241m=\u001b[39m \u001b[38;5;28msum\u001b[39m(all_proba) \u001b[38;5;241m/\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_estimators\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/sklearn/utils/parallel.py:67\u001b[0m, in \u001b[0;36mParallel.__call__\u001b[0;34m(self, iterable)\u001b[0m\n\u001b[1;32m     62\u001b[0m config \u001b[38;5;241m=\u001b[39m get_config()\n\u001b[1;32m     63\u001b[0m iterable_with_config \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m     64\u001b[0m     (_with_config(delayed_func, config), args, kwargs)\n\u001b[1;32m     65\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m delayed_func, args, kwargs \u001b[38;5;129;01min\u001b[39;00m iterable\n\u001b[1;32m     66\u001b[0m )\n\u001b[0;32m---> 67\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__call__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43miterable_with_config\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/joblib/parallel.py:2007\u001b[0m, in \u001b[0;36mParallel.__call__\u001b[0;34m(self, iterable)\u001b[0m\n\u001b[1;32m   2001\u001b[0m \u001b[38;5;66;03m# The first item from the output is blank, but it makes the interpreter\u001b[39;00m\n\u001b[1;32m   2002\u001b[0m \u001b[38;5;66;03m# progress until it enters the Try/Except block of the generator and\u001b[39;00m\n\u001b[1;32m   2003\u001b[0m \u001b[38;5;66;03m# reaches the first `yield` statement. This starts the asynchronous\u001b[39;00m\n\u001b[1;32m   2004\u001b[0m \u001b[38;5;66;03m# dispatch of the tasks to the workers.\u001b[39;00m\n\u001b[1;32m   2005\u001b[0m \u001b[38;5;28mnext\u001b[39m(output)\n\u001b[0;32m-> 2007\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m output \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mreturn_generator \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;43mlist\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43moutput\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/joblib/parallel.py:1650\u001b[0m, in \u001b[0;36mParallel._get_outputs\u001b[0;34m(self, iterator, pre_dispatch)\u001b[0m\n\u001b[1;32m   1647\u001b[0m     \u001b[38;5;28;01myield\u001b[39;00m\n\u001b[1;32m   1649\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backend\u001b[38;5;241m.\u001b[39mretrieval_context():\n\u001b[0;32m-> 1650\u001b[0m         \u001b[38;5;28;01myield from\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_retrieve()\n\u001b[1;32m   1652\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mGeneratorExit\u001b[39;00m:\n\u001b[1;32m   1653\u001b[0m     \u001b[38;5;66;03m# The generator has been garbage collected before being fully\u001b[39;00m\n\u001b[1;32m   1654\u001b[0m     \u001b[38;5;66;03m# consumed. This aborts the remaining tasks if possible and warn\u001b[39;00m\n\u001b[1;32m   1655\u001b[0m     \u001b[38;5;66;03m# the user if necessary.\u001b[39;00m\n\u001b[1;32m   1656\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_exception \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/joblib/parallel.py:1762\u001b[0m, in \u001b[0;36mParallel._retrieve\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1757\u001b[0m \u001b[38;5;66;03m# If the next job is not ready for retrieval yet, we just wait for\u001b[39;00m\n\u001b[1;32m   1758\u001b[0m \u001b[38;5;66;03m# async callbacks to progress.\u001b[39;00m\n\u001b[1;32m   1759\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m ((\u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jobs) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m) \u001b[38;5;129;01mor\u001b[39;00m\n\u001b[1;32m   1760\u001b[0m     (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jobs[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mget_status(\n\u001b[1;32m   1761\u001b[0m         timeout\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtimeout) \u001b[38;5;241m==\u001b[39m TASK_PENDING)):\n\u001b[0;32m-> 1762\u001b[0m     \u001b[43mtime\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msleep\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m0.01\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1763\u001b[0m     \u001b[38;5;28;01mcontinue\u001b[39;00m\n\u001b[1;32m   1765\u001b[0m \u001b[38;5;66;03m# We need to be careful: the job list can be filling up as\u001b[39;00m\n\u001b[1;32m   1766\u001b[0m \u001b[38;5;66;03m# we empty it and Python list are not thread-safe by\u001b[39;00m\n\u001b[1;32m   1767\u001b[0m \u001b[38;5;66;03m# default hence the use of the lock\u001b[39;00m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Necessary imports\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.ensemble import RandomForestClassifier, BaggingClassifier\n",
    "from sklearn.metrics import log_loss\n",
    "from sklearn.model_selection import PredefinedSplit, RandomizedSearchCV\n",
    "from skopt import BayesSearchCV\n",
    "from skopt.space import Integer, Real, Categorical\n",
    "from scipy.stats import randint, uniform\n",
    "\n",
    "added_features = ['log_diff', 'frac_diff', 'sadf']\n",
    "stock_vars = ['ret_1_0', 'prc_highprc_252d', 'seas_1_1na', 'rmax5_21d']  # List of features\n",
    "tgt_var = 'target'  # Target variable\n",
    "\n",
    "starting = pd.to_datetime(\"20000101\", format=\"%Y%m%d\")\n",
    "counter = 0\n",
    "pred_out = pd.DataFrame()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "X.index = pd.MultiIndex.from_tuples(\n",
    "    [(pd.to_datetime(t1_index), other_index) for t1_index, other_index in X.index]\n",
    ")\n",
    "Y.index = pd.MultiIndex.from_tuples(\n",
    "    [(pd.to_datetime(t1_index), other_index) for t1_index, other_index in Y.index]\n",
    ")\n",
    "\n",
    "# Estimation with expanding window\n",
    "while (starting + pd.DateOffset(years=11 + counter)) <= pd.to_datetime(\"20240101\", format=\"%Y%m%d\"):\n",
    "    # For testing purposes\n",
    "    # if counter == 1:\n",
    "    #     break\n",
    "\n",
    "    cutoff = [\n",
    "        starting,\n",
    "        starting + pd.DateOffset(years=8 + counter),  # Training set end date\n",
    "        starting + pd.DateOffset(years=10 + counter),  # Validation set end date\n",
    "        starting + pd.DateOffset(years=11 + counter),  # Test set end date\n",
    "    ]\n",
    "\n",
    "    print(f\"Train Start: {cutoff[0]}, Train End: {cutoff[1]}, Val Start: {cutoff[1]}, Val End: {cutoff[2]}, Test Start: {cutoff[2]}, Test End: {cutoff[3]}\")\n",
    "\n",
    "    # Cut the sample into training, validation, and testing sets\n",
    "    X_train = X[(X.index.get_level_values(0) >= cutoff[0]) & (X.index.get_level_values(0) < cutoff[1])]\n",
    "    X_validate = X[(X.index.get_level_values(0) >= cutoff[1]) & (X.index.get_level_values(0) < cutoff[2])]\n",
    "    X_test = X[(X.index.get_level_values(0) >= cutoff[2]) & (X.index.get_level_values(0) < cutoff[3])]\n",
    "\n",
    "    Y_train = Y[(Y.index.get_level_values(0) >= cutoff[0]) & (Y.index.get_level_values(0) < cutoff[1])]\n",
    "    Y_validate = Y[(Y.index.get_level_values(0) >= cutoff[1]) & (Y.index.get_level_values(0) < cutoff[2])]\n",
    "    Y_test = Y[(Y.index.get_level_values(0) >= cutoff[2]) & (Y.index.get_level_values(0) < cutoff[3])]\n",
    "\n",
    "    # Adjust sample weights\n",
    "    Y_train['weight_attr'] *= Y_train.shape[0] / Y_train['weight_attr'].sum()\n",
    "    Y_validate['weight_attr'] *= Y_validate.shape[0] / Y_validate['weight_attr'].sum()\n",
    "    Y_test['weight_attr'] *= Y_test.shape[0] / Y_test['weight_attr'].sum()\n",
    "\n",
    "\n",
    "    X_train_vals = X_train[stock_vars].values\n",
    "    X_validate_vals = X_validate[stock_vars].values\n",
    "    X_test_vals = X_test[stock_vars].values\n",
    "\n",
    "    X_train_val = np.vstack([X_train_vals, X_validate_vals])\n",
    "    Y_train_val = pd.concat([Y_train, Y_validate])\n",
    "\n",
    "    # Create test_fold \n",
    "    test_fold = np.concatenate([\n",
    "        np.full(len(X_train_vals), -1),  # training set indices\n",
    "        np.zeros(len(X_validate_vals))   # validation set indices\n",
    "    ])\n",
    "    ps = PredefinedSplit(test_fold)\n",
    "\n",
    "\n",
    "    base_rf = RandomForestClassifier(\n",
    "        criterion=\"entropy\",\n",
    "        bootstrap=False,\n",
    "        class_weight=\"balanced_subsample\"\n",
    "    )\n",
    "\n",
    "\n",
    "    bagging_clf = BaggingClassifier(\n",
    "        estimator=base_rf,\n",
    "        oob_score=True,\n",
    "        n_jobs=-1\n",
    "    )\n",
    "\n",
    " \n",
    "    param_distributions = {\n",
    "    'estimator__n_estimators': randint(10, 1000),\n",
    "    'estimator__max_depth': randint(5, 50),\n",
    "    'estimator__min_samples_split': randint(2, 10),\n",
    "    'estimator__min_samples_leaf': randint(1, 5),\n",
    "    'estimator__max_features': ['sqrt', 'log2'],  \n",
    "    'n_estimators': randint(10, 100),  \n",
    "    'max_samples': uniform(0.1, 1.0),  \n",
    "    'max_features': uniform(0.1, 1.0)  \n",
    "    }\n",
    "\n",
    "    # Define the optimizer\n",
    "    optimizer = RandomizedSearchCV(\n",
    "    bagging_clf,\n",
    "    param_distributions=param_distributions,\n",
    "    n_iter=1,  \n",
    "    cv=ps,      # Use predefined split\n",
    "    n_jobs=-1,\n",
    "    verbose=2,\n",
    "    random_state=42,\n",
    "    scoring='neg_log_loss'\n",
    "    )\n",
    "\n",
    "\n",
    "    optimizer.fit(\n",
    "        X_train_val,\n",
    "        Y_train_val[tgt_var].values,\n",
    "        **{'sample_weight': Y_train_val['weight_attr'].values}\n",
    "    )\n",
    "\n",
    "\n",
    "    best_estimator = optimizer.best_estimator_\n",
    "\n",
    "\n",
    "    best_estimator.fit(\n",
    "        X_train_val,\n",
    "        Y_train_val[tgt_var].values,\n",
    "        sample_weight=Y_train_val['weight_attr'].values\n",
    "    )\n",
    "\n",
    "    # Predict on test set\n",
    "    prob = best_estimator.predict_proba(X_test_vals)\n",
    "    score_ = -log_loss(Y_test[tgt_var].values, prob, sample_weight=Y_test[\"weight_attr\"].values, labels=best_estimator.classes_)\n",
    "    print(\"Log Loss on Test Set:\", score_)\n",
    "\n",
    "    # Store predictions in Y_test\n",
    "    Y_test['prediction'] = best_estimator.predict(X_test_vals)\n",
    "    Y_test['probability'] = prob.max(axis=1)\n",
    "\n",
    "    # print(\"Predictions on Test Set:\")\n",
    "    # print(Y_test['prediction'])\n",
    "    # print(\"Probabilities on Test Set:\")\n",
    "    # print(Y_test['probability'])\n",
    "    # print(\"Prediction Probabilities:\")\n",
    "    # print(prob)\n",
    "\n",
    " \n",
    "    pred_out = pred_out.append(Y_test[['prediction', 'probability']])\n",
    "    pred_out.to_csv(\"../objects/predictions.csv\")\n",
    "\n",
    "\n",
    "    counter += 1\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## THE CLASSIFIER"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can use one of the following. For clarification, refer to **6.4**\n",
    "\n",
    "I believe clf2 is the best one\n",
    "\n",
    "### This is from my deprado repo, *ch_06.ipynb*\n",
    "\n",
    "```python\n",
    "# Libraries\n",
    "from sklearn.ensemble import BaggingClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.datasets import make_classification\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Dataset\n",
    "X, Y = f_ch8.getTestData(n_features=40, n_informative=3, n_redundant=30, n_samples=10000)\n",
    "Y = Y[\"bin\"]\n",
    "\n",
    "# usual RF\n",
    "clf0 = RandomForestClassifier(\n",
    "    n_estimators=1_000,  # 1_000 trees\n",
    "    class_weight=\"balanced_subsample\",  # prevent minority class from being ignored\n",
    "    criterion=\"entropy\"  # information gain\n",
    ")\n",
    "\n",
    "# Ensemble of estimators with base estimator as a decision tree\n",
    "clf1 = DecisionTreeClassifier(\n",
    "    criterion=\"entropy\",  # information gain\n",
    "    max_features=\"sqrt\",  # sqrt(n_features) to force diversity among trees\n",
    "    class_weight=\"balanced\"  # prevent minority class from being ignored\n",
    ")\n",
    "clf1 = BaggingClassifier(\n",
    "    estimator=clf1,  # base estimator\n",
    "    n_estimators=1_000,  # 1_000 trees\n",
    "    max_samples=0.6,  # average uniqueness\n",
    "    max_features=1.0  # all features for bagging\n",
    ")\n",
    "\n",
    "# Bagging classifier on RF where max_samples is set to average uniqueness\n",
    "clf2 = RandomForestClassifier(\n",
    "    n_estimators=1,  # 1 tree\n",
    "    criterion=\"entropy\",  # information gain\n",
    "    bootstrap=False,  # no bootstrap\n",
    "    class_weight=\"balanced_subsample\"  # prevent minority class from being ignored\n",
    ")\n",
    "\n",
    "clf2 = BaggingClassifier(\n",
    "    estimator=clf2,  # base estimator\n",
    "    n_estimators=1_000,  # 1_000 trees\n",
    "    max_samples=1.0,  # average uniqueness\n",
    "    max_features=1.0  # all features for bagging\n",
    ")\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hyperparameter Tunning\n",
    "\n",
    "### Here is why the weights are so important\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Refer to *9.4* , you can use accuracy, but negative log likelihood is suggested,\\\n",
    "since it takes into account the probability of the prediction."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the hyperparaemtes, i believe you guys can figure out better than me what\n",
    "to set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Please be aware of the bugs mentioned in\n",
    "\n",
    "- 7.5 BUGS IN SKLEARN’S CROSS-VALIDATION\n",
    "- SNIPPET 9.2 AN ENHANCED PIPELINE CLASS\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## I'M AVAILABLE ALL DAY, PLEASE ASK ME ANYTHING, LET'S GOOOOO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "financial_math",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
