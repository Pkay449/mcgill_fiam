{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext watermark\n",
    "%watermark\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import pickle\n",
    "\n",
    "import sys\n",
    "sys.path.append('../src/ch_08')\n",
    "\n",
    "import code_ch_08 as f_ch8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load objects/X_dataset.pkl\n",
    "with open('../objects/X_dataset.pkl', 'rb') as f:\n",
    "    X = pickle.load(f)\n",
    "    \n",
    "with open('../objects/stacked_data.pkl', 'rb') as f:\n",
    "    stacked_data = pickle.load(f)\n",
    "    \n",
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = '../raw_data/factor_char_list.csv'\n",
    "features = pd.read_csv(path)\n",
    "features_list = features.values.ravel().tolist()\n",
    "features_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Investigate NAs in X\n",
    "\n",
    "# NAs per column\n",
    "nas = X.isna().sum()\n",
    "nas.sort_values(ascending=False, inplace=True)\n",
    "nas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Group by year, and count NAs\n",
    "# X does not have 'year', so we use 'year' from stacked_data\n",
    "X['year'] = stacked_data['year']\n",
    "nas_year = X.groupby('year').apply(lambda x: x.isna().sum())\n",
    "nas_year"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check per stock\n",
    "X['ticker'] = stacked_data['stock_ticker']\n",
    "nas_stock = X.groupby('ticker').apply(lambda x: x.isna().sum())\n",
    "nas_stock"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For each feature, count the number of stoks (rows) with NAs\n",
    "nas_stock_count=(nas_stock>0).sum().sort_values(ascending=False)\n",
    "nas_stock_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nas_feature_stock_count = (nas_stock_count>0).sum()\n",
    "nas_feature_stock_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Group by year and ticker, then count the number of NAs (missing values) for each feature\n",
    "nas_feature_year_stock = X.groupby(['year', 'ticker']).apply(lambda group: group.isna().sum()>0)\n",
    "\n",
    "# Display the result\n",
    "print(nas_feature_year_stock)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Group by the 'year' index level and sum the boolean values\n",
    "nas_feature_year = nas_feature_year_stock.groupby(level='year').sum()\n",
    "\n",
    "# Display the result\n",
    "print(nas_feature_year)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Remove NAs from X_dataset and run feature importance scripts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_clean = X.copy()\n",
    "X_clean.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Record count BEFORE dropping NaN records: \", len(X_clean))\n",
    "X_clean.dropna(inplace=True)\n",
    "X_clean.drop(columns=['ticker'], inplace=True)\n",
    "print(\"Record count AFTER dropping NaN records: \", len(X_clean))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SNIPPET 8.5 COMPUTATION OF ORTHOGONAL FEATURES (Modified for Variance and Loadings)\n",
    "def get_eVec(dot, varThres):\n",
    "    # Compute eigenvalues (eVal) and eigenvectors (eVec) from dot product matrix\n",
    "    eVal, eVec = np.linalg.eigh(dot)\n",
    "    \n",
    "    # Sort eigenvalues and eigenvectors in descending order\n",
    "    idx = eVal.argsort()[::-1]  # Sort eigenvalues in descending order\n",
    "    eVal, eVec = eVal[idx], eVec[:, idx]\n",
    "    \n",
    "    # Keep only positive eigenvalues\n",
    "    eVal = pd.Series(eVal, index=[\"PC_\" + str(i + 1) for i in range(eVal.shape[0])])\n",
    "    eVec = pd.DataFrame(eVec, index=dot.index, columns=eVal.index)\n",
    "    \n",
    "    # Compute cumulative variance explained\n",
    "    cumVar = eVal.cumsum() / eVal.sum()\n",
    "    \n",
    "    # Select the number of principal components that explain at least varThres variance\n",
    "    dim = cumVar.values.searchsorted(varThres)\n",
    "    \n",
    "    # Keep only the selected principal components\n",
    "    eVal, eVec = eVal.iloc[: dim + 1], eVec.iloc[:, : dim + 1]\n",
    "    \n",
    "    # Return eigenvalues (variance explained) and eigenvectors (loadings)\n",
    "    return eVal, eVec, cumVar.iloc[: dim + 1]\n",
    "\n",
    "\n",
    "# Function to standardize features and compute orthogonal features (PCA)\n",
    "def orthoFeats(dfX, varThres=0.95):\n",
    "    # Standardize the feature matrix\n",
    "    dfZ = dfX.sub(dfX.mean(), axis=1).div(dfX.std(), axis=1)\n",
    "    \n",
    "    # Compute the dot product (covariance matrix)\n",
    "    dot = pd.DataFrame(np.dot(dfZ.T, dfZ), index=dfX.columns, columns=dfX.columns)\n",
    "    \n",
    "    # Get eigenvalues (variance explained) and eigenvectors (loadings)\n",
    "    eVal, eVec, cumVar = get_eVec(dot, varThres)\n",
    "    \n",
    "    # Transform the original features into the new principal components\n",
    "    dfP = np.dot(dfZ, eVec)\n",
    "    \n",
    "    return dfP, eVal, eVec, cumVar\n",
    "\n",
    "\n",
    "# Apply the function to your dataset\n",
    "X_pca, eigenvalues, loadings, cumulative_variance = orthoFeats(X_clean, varThres=0.95)\n",
    "\n",
    "# Convert PCA-transformed data into a DataFrame with appropriate column names\n",
    "X_pca = pd.DataFrame(X_pca, index=X_clean.index)\n",
    "X_pca.columns = [\"pca_%d\" % i for i in range(X_pca.shape[1])]\n",
    "\n",
    "# Print the variance explained by each principal component (eigenvalues)\n",
    "print(\"Variance Explained (Eigenvalues):\")\n",
    "print(eigenvalues)\n",
    "\n",
    "# Print the cumulative variance explained\n",
    "print(\"\\nCumulative Variance Explained:\")\n",
    "print(cumulative_variance)\n",
    "\n",
    "# Print the loadings (eigenvectors)\n",
    "print(\"\\nLoadings (Eigenvectors):\")\n",
    "print(loadings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now X_pca contains the PCA-transformed features, eigenvalues contain variance explained,\n",
    "# and loadings give the contribution of each original feature to each principal component.\n",
    "\n",
    "variance_explained = eigenvalues / eigenvalues.sum()\n",
    "\n",
    "\n",
    "# Function to get top important features based on variance explained and loadings\n",
    "def get_top_features(variance_explained, loadings, top_n=20):\n",
    "    \"\"\"\n",
    "    Rank features by their importance using the variance explained by each principal component \n",
    "    and the absolute value of the feature's loadings.\n",
    "    \n",
    "    Arguments:\n",
    "    - variance_explained: Series, variance explained by each principal component.\n",
    "    - loadings: DataFrame, loadings (eigenvectors) where columns are principal components and rows are features.\n",
    "    - top_n: Number of top features to return.\n",
    "    \n",
    "    Returns:\n",
    "    - ranked_features: DataFrame with features ranked by importance.\n",
    "    \"\"\"\n",
    "    # Ensure the absolute values of the loadings are used\n",
    "    abs_loadings = loadings.abs()\n",
    "    \n",
    "    # Multiply each feature's loading by the variance explained of the respective principal component\n",
    "    feature_importance = abs_loadings.mul(variance_explained, axis=1)\n",
    "    \n",
    "    # Sum the weighted contributions across all principal components for each feature\n",
    "    feature_importance['total_importance'] = feature_importance.sum(axis=1)\n",
    "    \n",
    "    # Sort features by their total importance in descending order\n",
    "    ranked_features = feature_importance[['total_importance']].sort_values(by='total_importance', ascending=False)\n",
    "    \n",
    "    # Return the top N important features\n",
    "    return ranked_features.head(top_n)\n",
    "\n",
    "# Apply the function to your data\n",
    "top_20_features = get_top_features(variance_explained, loadings, top_n=20)\n",
    "\n",
    "# Print the top 20 important features\n",
    "print(\"Top 20 Important Features:\")\n",
    "print(top_20_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_pca = f_ch8.orthoFeats(X_clean)\n",
    "X_pca = pd.DataFrame(X_pca, index=X_clean.index)\n",
    "# name each column \"pca_i\" where i is the index of the column\n",
    "X_pca.columns = [\"pca_%d\" % i for i in range(X_pca.shape[1])]\n",
    "X_pca"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Calculate explained variance (if applicable)\n",
    "# from sklearn.decomposition import PCA\n",
    "# pca = PCA()\n",
    "# pca.fit(X_clean)\n",
    "# explained_variance = pca.explained_variance_ratio_\n",
    "\n",
    "# # Display the explained variance by each principal component\n",
    "# print(\"Explained Variance by Principal Component:\")\n",
    "# for i, var in enumerate(explained_variance, 1):\n",
    "#     print(f\"PC{i}: {var:.4f}\")\n",
    "\n",
    "# # Loadings: how much each feature contributes to each principal component\n",
    "# pca_loadings = pd.DataFrame(pca.components_.T, index=X_clean.columns, columns=[f'PC{i+1}' for i in range(len(X_clean.columns))])\n",
    "\n",
    "# print(\"PCA Loadings (Feature Contributions to Components):\")\n",
    "# print(pca_loadings)\n",
    "\n",
    "# # Visualize the loadings with a heatmap\n",
    "# import seaborn as sns\n",
    "# import matplotlib.pyplot as plt\n",
    "\n",
    "# plt.figure(figsize=(10, 6))\n",
    "# sns.heatmap(pca_loadings, annot=True, cmap=\"coolwarm\", center=0)\n",
    "# plt.title(\"PCA Loadings Heatmap\")\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the indices that were dropped from X_clean\n",
    "dropped_indices = X.index.difference(X_clean.index)\n",
    "cont = pd.concat([stacked_data['datetime'], stacked_data['target'], stacked_data['weight_attr']], axis=1, ignore_index=True)\n",
    "cont.rename(columns={cont.columns[0]: 't1', cont.columns[1]: 'bin', cont.columns[2]: 'w'}, inplace=True)\n",
    "cont"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cont = cont.drop(dropped_indices)\n",
    "cont"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# t1 = pd.Series()\n",
    "tmp = cont['t1'].shift(-1).dropna()\n",
    "tmp = pd.to_datetime(tmp)\n",
    "# last date\n",
    "result = tmp.iloc[-1] + pd.DateOffset(days=5) + pd.tseries.offsets.BMonthEnd(1)\n",
    "tmp = pd.concat([tmp, pd.Series([result])], ignore_index=True)\n",
    "# t1 = tmp\n",
    "# index as first business day of the following month\n",
    "# t1.index = pd.to_datetime(datetime) + pd.DateOffset(days=5) - pd.tseries.offsets.BMonthBegin(1)\n",
    "cont.index = pd.to_datetime(cont['t1']) - pd.DateOffset(days=40) + pd.tseries.offsets.BMonthEnd(1)\n",
    "cont"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_clean['datetime'] = cont.index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_clean.set_index('datetime', inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cont['w'] *= cont.shape[0]/cont['w'].sum()\n",
    "cont"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cont.isna().any(axis=1).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Libraries\n",
    "from sklearn.ensemble import BaggingClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.datasets import make_classification\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Bagging classifier on RF where max_samples is set to average uniqueness\n",
    "clf2 = RandomForestClassifier(\n",
    "    n_estimators=1,  # 1 tree\n",
    "    criterion=\"entropy\",  # information gain\n",
    "    bootstrap=False,  # no bootstrap\n",
    "    class_weight=\"balanced_subsample\"  # prevent minority class from being ignored\n",
    ")\n",
    "\n",
    "clf2 = BaggingClassifier(\n",
    "    estimator=clf2,  # base estimator\n",
    "    n_estimators=1_000,  # 1_000 trees\n",
    "    max_samples=0.94,  # average uniqueness\n",
    "    max_features=1.0  # all features for bagging\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "methods = ['MDI', 'MDA', 'SFI']\n",
    "\n",
    "n_estimators = 1000  # Number of trees in the random forest\n",
    "cv = 10  # Number of cross-validation folds\n",
    "max_samples = 1.0  # Use the entire dataset for each tree\n",
    "numThreads = 1  # Adjust based on your available computational resources\n",
    "pctEmbargo = 0  # No embargo for simplicity\n",
    "\n",
    "for method in methods:\n",
    "    print(f\"Running feature importance for {method}...\")\n",
    "    imp, oob, oos = f_ch8.featImportance(pd.DataFrame(X_clean), cont, n_estimators=n_estimators, cv=cv,\n",
    "                                    max_samples=max_samples, numThreads=numThreads, \n",
    "                                    pctEmbargo=pctEmbargo, method=method)\n",
    "    \n",
    "    # Plot the feature importance using the provided function\n",
    "    f_ch8.plotFeatImportance(pathOut='./', imp=imp, oob=oob, oos=oos, method=method, tag='test', simNum=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "financial_math",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
