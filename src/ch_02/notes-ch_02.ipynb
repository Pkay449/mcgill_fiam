{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports\n",
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## p 29"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\begin{align*}\n",
    "\\theta_{T} &= \\sum_{t = 1}^{T} b_{t} \\\\\n",
    "&\\text{The expected value, at the beggining is} \\\\\n",
    "E_{0}[\\theta_{T}] &= E_{0}[T]\\ (2 * P(b_t = 1) - 1) \\\\\n",
    "&\\text{ We define a tick imbalance bar (TIB) as } \\theta_{T^{*}} \\text{ where} \\\\\n",
    "T^{*} &= \\arg \\min_{T} \\{ \\left| \\theta_{T} \\right| \\geq\\ \\left| E_{0}[\\theta_{T}] \\right|\\ \\} \\\\\n",
    "&= \\arg \\min_{T} \\{ \\left| \\theta_{T} \\right| \\geq\\ E_{0}[T]\\ \\left| 2 * P(b_t = 1) - 1 \\right|\\ \\} \\\\\n",
    "\\end{align*}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\begin{align*}\n",
    "E_{0}[\\theta_{T}] &: \\text{Expected size of the tick bar.} \\\\\n",
    "|2 * P(b_t = 1) - 1| &: \\text{The measure on what the size of the expected imbalance} \\\\\n",
    "& \\text{\\hspace{2.2em}is implied on.}\n",
    "\\end{align*}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From my understanding, the purpose of TIBs, DIBs and VIBs is that, in the case\\\n",
    "where we are given access to high frequency data, in the form of ticks, it is\\\n",
    "much more efficient to sample bars, which are groups of ticks and sampled in a\\\n",
    "way that the information is not lost.\n",
    "\n",
    "Yes, your understanding is essentially correct. Tick Imbalance Bars (TIBs), Dollar Imbalance Bars (DIBs), and Volume Imbalance Bars (VIBs) are methods used in the analysis of high-frequency trading data that aim to sample data more intelligently based on the arrival of new information to the market. These methods are part of a broader category known as \"information-driven bars,\" which are designed to overcome the limitations of time-based sampling in financial data analysis.\n",
    "\n",
    "Here's a breakdown of each type of bar and the philosophy behind them:\n",
    "\n",
    "### Tick Imbalance Bars (TIBs)\n",
    "- **Purpose**: TIBs are created by grouping ticks until a certain imbalance in the trade direction is reached. The imbalance is measured by a predefined threshold.\n",
    "- **Rationale**: The idea is that ticks (individual trades) carry information about market direction based on the volume and price changes. By grouping ticks into bars when there is a significant imbalance, you capture moments when potentially informed traders are active, thus capturing periods of significant market moves before they are fully realized.\n",
    "\n",
    "### Volume Imbalance Bars (VIBs)\n",
    "- **Purpose**: Similar to TIBs but with a focus on the volume of trades. VIBs group ticks until the accumulated volume of buys versus sells exceeds a threshold.\n",
    "- **Rationale**: This method assumes that large volumes signify meaningful market activity, particularly informed trading. By focusing on volume imbalances, these bars aim to capture significant market events driven by volume shifts.\n",
    "\n",
    "### Dollar Imbalance Bars (DIBs)\n",
    "- **Purpose**: These bars extend the concept to the dollar value of trades, grouping ticks until the dollar imbalance (buy dollar volume vs. sell dollar volume) exceeds a threshold.\n",
    "- **Rationale**: The dollar value can provide a more accurate measure of market impact, especially in asset classes where trade sizes vary significantly. This method helps in identifying bars that represent significant monetary movements in the market, potentially signaling major strategic trades.\n",
    "\n",
    "### Common Benefits of Information-Driven Bars\n",
    "- **Efficiency in Sampling**: Unlike time-based bars (like every minute or hour), which treat quiet periods and busy periods the same, information-driven bars focus on periods of significant activity. This results in more efficient use of data, as more bars are sampled during periods of high information flow and fewer during quiet times.\n",
    "- **Reduction in Noise**: These methods help reduce noise by focusing on data points that have actual information content, potentially leading to better trading signals.\n",
    "- **Adaptability**: These bars adapt to market conditions dynamically, allowing for variable bar sizes based on underlying market activity. This is particularly useful in avoiding the pitfalls of fixed-size bars that might either over-sample during quiet periods or under-sample during active periods.\n",
    "\n",
    "In summary, the purpose of using TIBs, VIBs, and DIBs is to ensure that the sampling of financial data into bars is driven by the informational content of the trades rather than arbitrary time intervals. This approach aims to capture the essence of market dynamics more accurately and efficiently, particularly beneficial in algorithmic trading and quantitative analysis."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2.4.2 PCA Weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pcaWeights(cov, riskDist=None, riskTarget=1.):\n",
    "    # Following the riskAlloc distribution, match riskTarget\n",
    "    eVal, eVec = np.linalg.eigh(cov)  # must be Hermitian\n",
    "    indices = eVal.argsort()[::-1]  # arguments for sorting eVal descending\n",
    "    eVal, eVec = eVal[indices], eVec[:, indices]\n",
    "    if riskDist is None:\n",
    "        riskDist = np.zeros(cov.shape[0])\n",
    "        riskDist[-1] = 1.\n",
    "    loads = riskTarget * (riskDist / eVal) ** 0.5\n",
    "    wghts = np.dot(eVec, np.reshape(loads, (-1, 1)))\n",
    "    # ctr = (loads / riskTarget) ** 2 * eVal  # verify riskDist\n",
    "    return wghts\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Certainly! Let's break down the different inputs to the `pcaWeights` function and explore how they work and interact with each other. Here are the inputs:\n",
    "\n",
    "1. **cov**: This is the covariance matrix of the asset returns. It is a square matrix that contains the covariances between each pair of assets. The size of the matrix is \\( N \\times N \\), where \\( N \\) is the number of assets.\n",
    "2. **riskDist**: This is an optional input. It is an array that specifies the desired distribution of risk across the principal components. If not provided, the function assumes all risk is allocated to the component with the smallest eigenvalue.\n",
    "3. **riskTarget**: This is a scalar that represents the target risk level for the portfolio. It scales the overall risk allocation.\n",
    "\n",
    "### Interpreting Different Sets of Inputs\n",
    "\n",
    "#### Case 1: Basic Example\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-1.05868598]\n",
      " [-2.49416586]\n",
      " [ 3.14066144]]\n"
     ]
    }
   ],
   "source": [
    "cov_matrix = np.array([[0.1, 0.02, 0.03], [0.02, 0.1, 0.04], [0.03, 0.04, 0.1]])\n",
    "weights = pcaWeights(cov_matrix)\n",
    "print(weights)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "- **cov**: The covariance matrix provided here is a simple 3x3 matrix representing three assets.\n",
    "- **riskDist**: Not provided, so it defaults to allocating all risk to the principal component with the smallest eigenvalue.\n",
    "- **riskTarget**: Default value of 1.0.\n",
    "\n",
    "**Interpretation**:\n",
    "- The function calculates the PCA weights such that all risk is allocated to the component with the smallest eigenvalue. This will likely result in weights that minimize variance by focusing on the least risky component.\n",
    "\n",
    "#### Case 2: Specifying Risk Distribution\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-2.89734397]\n",
      " [-1.36761731]\n",
      " [ 1.81096786]]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "cov_matrix = np.array([[0.1, 0.02, 0.03], [0.02, 0.1, 0.04], [0.03, 0.04, 0.1]])\n",
    "risk_distribution = np.array([0.2, 0.3, 0.5])\n",
    "weights = pcaWeights(cov_matrix, riskDist=risk_distribution)\n",
    "print(weights)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "- **cov**: The same covariance matrix as before.\n",
    "- **riskDist**: Specifies that 20% of the risk should be allocated to the first principal component, 30% to the second, and 50% to the third.\n",
    "- **riskTarget**: Default value of 1.0.\n",
    "\n",
    "**Interpretation**:\n",
    "- The function calculates the weights such that the specified risk distribution is achieved. This means the portfolio will be constructed to align with the desired risk proportions across the principal components.\n",
    "\n",
    "#### Case 3: Adjusting the Risk Target\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-1.44867198]\n",
      " [-0.68380866]\n",
      " [ 0.90548393]]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "cov_matrix = np.array([[0.1, 0.02, 0.03], [0.02, 0.1, 0.04], [0.03, 0.04, 0.1]])\n",
    "risk_distribution = np.array([0.2, 0.3, 0.5])\n",
    "weights = pcaWeights(cov_matrix, riskDist=risk_distribution, riskTarget=0.5)\n",
    "print(weights)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "- **cov**: The same covariance matrix as before.\n",
    "- **riskDist**: Specifies the same risk distribution as before.\n",
    "- **riskTarget**: Set to 0.5, meaning the overall risk level is halved.\n",
    "\n",
    "**Interpretation**:\n",
    "- The function scales down the overall risk by 50%, resulting in a portfolio that has the same risk distribution but at a lower total risk level.\n",
    "\n",
    "### Interaction Between Inputs\n",
    "\n",
    "1. **cov and riskDist**:\n",
    "   - The covariance matrix determines the principal components and their associated eigenvalues. The risk distribution specifies how much risk should be allocated to each of these components.\n",
    "   - If `riskDist` is not provided, the function defaults to allocating all risk to the component with the smallest eigenvalue, which typically represents the least risk.\n",
    "\n",
    "2. **cov and riskTarget**:\n",
    "   - The covariance matrix defines the structure of risks and returns among the assets. The risk target scales this structure up or down.\n",
    "   - A higher `riskTarget` means a higher overall risk, while a lower `riskTarget` means a lower overall risk.\n",
    "\n",
    "3. **riskDist and riskTarget**:\n",
    "   - The risk distribution specifies the proportions of risk allocated to each component, while the risk target scales the total risk.\n",
    "   - Together, they determine both the proportions and the total level of risk in the portfolio.\n",
    "\n",
    "### Examples with Explanations\n",
    "\n",
    "1. **Equal Risk Distribution**\n",
    "\n",
    " \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "   risk_distribution = np.array([1/3, 1/3, 1/3])\n",
    "   weights = pcaWeights(cov_matrix, riskDist=risk_distribution)\n",
    "````"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "   **Interpretation**:\n",
    "   - Equal risk is allocated to each of the three principal components, resulting in a balanced risk profile.\n",
    "\n",
    "2. **Conservative Risk Distribution**\n",
    "\n",
    "   ```python\n",
    "   risk_distribution = np.array([0.1, 0.2, 0.7])\n",
    "   weights = pcaWeights(cov_matrix, riskDist=risk_distribution, riskTarget=0.3)\n",
    "   ```\n",
    "\n",
    "   **Interpretation**:\n",
    "   - Most of the risk is allocated to the third principal component, which might represent the least volatile or most stable factor. The overall risk is scaled down to 30%.\n",
    "\n",
    "3. **Aggressive Risk Distribution**\n",
    "\n",
    "   ```python\n",
    "   risk_distribution = np.array([0.5, 0.3, 0.2])\n",
    "   weights = pcaWeights(cov_matrix, riskDist=risk_distribution, riskTarget=1.5)\n",
    "   ```\n",
    "\n",
    "   **Interpretation**:\n",
    "   - More risk is allocated to the first principal component, which could be the most volatile or highest return factor. The overall risk is scaled up to 150%.\n",
    "\n",
    "By adjusting these inputs, a portfolio manager can tailor the risk profile of the portfolio to match specific investment strategies or risk tolerances."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2.4.3 Single Futures Roll"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "applied-math-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
