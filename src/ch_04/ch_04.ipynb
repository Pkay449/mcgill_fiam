{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Last updated: 2024-09-18T12:34:30.117873-04:00\n",
      "\n",
      "Python implementation: CPython\n",
      "Python version       : 3.8.19\n",
      "IPython version      : 8.12.2\n",
      "\n",
      "Compiler    : Clang 16.0.6 \n",
      "OS          : Darwin\n",
      "Release     : 23.6.0\n",
      "Machine     : arm64\n",
      "Processor   : arm\n",
      "CPU cores   : 8\n",
      "Architecture: 64bit\n",
      "\n",
      "\n",
      "pandas           : 2.0.3\n",
      "pandas_datareader: 0.10.0\n",
      "dask             : 2023.5.0\n",
      "numpy            : 1.24.4\n",
      "sklearn          : 1.3.2\n",
      "statsmodels      : 0.14.1\n",
      "scipy            : 1.10.1\n",
      "ffn              : not installed\n",
      "matplotlib       : 3.7.3\n",
      "seaborn          : 0.13.2\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/j0/8v9qcjfx15g5ftmsy5n0qrq80000gn/T/ipykernel_39206/178535154.py:46: MatplotlibDeprecationWarning: The seaborn styles shipped by Matplotlib are deprecated since 3.6, as they no longer correspond to the styles shipped by seaborn. However, they will remain available as 'seaborn-v0_8-<style>'. Alternatively, directly use the seaborn API instead.\n",
      "  plt.style.use('seaborn-talk')\n"
     ]
    }
   ],
   "source": [
    "%load_ext watermark\n",
    "%watermark\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "\n",
    "# import standard libs\n",
    "from IPython.display import display\n",
    "from IPython.core.debugger import set_trace as bp\n",
    "from pathlib import PurePath, Path\n",
    "import sys\n",
    "import time\n",
    "from collections import OrderedDict as od\n",
    "import re\n",
    "import os\n",
    "import json\n",
    "import datetime\n",
    "import pickle\n",
    "\n",
    "\n",
    "# import python scientific stack\n",
    "import pandas as pd\n",
    "import pandas_datareader.data as web\n",
    "pd.set_option('display.max_rows', 100)\n",
    "from dask import dataframe as dd\n",
    "from dask.diagnostics import ProgressBar\n",
    "from multiprocessing import cpu_count\n",
    "pbar = ProgressBar()\n",
    "pbar.register()\n",
    "import numpy as np\n",
    "import scipy.stats as stats\n",
    "import statsmodels.api as sm\n",
    "from numba import jit\n",
    "import math\n",
    "# import ffn\n",
    "\n",
    "\n",
    "# import visual tools\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.gridspec as gridspec\n",
    "%matplotlib inline\n",
    "import seaborn as sns\n",
    "\n",
    "plt.style.use('seaborn-talk')\n",
    "plt.style.use('bmh')\n",
    "#plt.rcParams['font.family'] = 'DejaVu Sans Mono'\n",
    "plt.rcParams['font.size'] = 9.5\n",
    "plt.rcParams['font.weight'] = 'medium'\n",
    "plt.rcParams['figure.figsize'] = 10,7\n",
    "blue, green, red, purple, gold, teal = sns.color_palette('colorblind', 6)\n",
    "\n",
    "RANDOM_STATE = 777\n",
    "\n",
    "print()\n",
    "%watermark -p pandas,pandas_datareader,dask,numpy,sklearn,statsmodels,scipy,ffn,matplotlib,seaborn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Project root added to sys.path: /Users/paulkelendji/Desktop/GitHub_paul/ML-Asset_Management\n",
      "Config path added to sys.path: /Users/paulkelendji/Desktop/GitHub_paul/ML-Asset_Management/config\n",
      "Current sys.path: ['/Users/paulkelendji/miniconda3/envs/financial_math/lib/python38.zip', '/Users/paulkelendji/miniconda3/envs/financial_math/lib/python3.8', '/Users/paulkelendji/miniconda3/envs/financial_math/lib/python3.8/lib-dynload', '', '/Users/paulkelendji/miniconda3/envs/financial_math/lib/python3.8/site-packages', '/Users/paulkelendji/miniconda3/envs/financial_math/lib/python3.8/site-packages/setuptools/_vendor', '/Users/paulkelendji/Desktop/GitHub_paul/ML-Asset_Management', '/Users/paulkelendji/Desktop/GitHub_paul/ML-Asset_Management/config']\n",
      "Project root added to sys.path: /Users/paulkelendji/Desktop/GitHub_paul/ML-Asset_Management\n",
      "Config path added to sys.path: /Users/paulkelendji/Desktop/GitHub_paul/ML-Asset_Management/config\n",
      "Current sys.path: ['/Users/paulkelendji/miniconda3/envs/financial_math/lib/python38.zip', '/Users/paulkelendji/miniconda3/envs/financial_math/lib/python3.8', '/Users/paulkelendji/miniconda3/envs/financial_math/lib/python3.8/lib-dynload', '', '/Users/paulkelendji/miniconda3/envs/financial_math/lib/python3.8/site-packages', '/Users/paulkelendji/miniconda3/envs/financial_math/lib/python3.8/site-packages/setuptools/_vendor', '/Users/paulkelendji/Desktop/GitHub_paul/ML-Asset_Management', '/Users/paulkelendji/Desktop/GitHub_paul/ML-Asset_Management/config', '/Users/paulkelendji/Desktop/GitHub_paul/ML-Asset_Management', '/Users/paulkelendji/Desktop/GitHub_paul/ML-Asset_Management', '/Users/paulkelendji/Desktop/GitHub_paul/ML-Asset_Management', '/Users/paulkelendji/Desktop/GitHub_paul/ML-Asset_Management/config']\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "# Run the setup script\n",
    "%run ../../config/setup_project.py\n",
    "\n",
    "# Call the function to set up the project path\n",
    "setup_project_path()\n",
    "\n",
    "# Now you can import your modules\n",
    "from src.utils import helper as h_\n",
    "import src.ch_02.code_ch_02 as f_ch2\n",
    "import src.ch_03.code_ch_03 as f_ch3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercises"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>time_open</th>\n",
       "      <th>open</th>\n",
       "      <th>low</th>\n",
       "      <th>high</th>\n",
       "      <th>close</th>\n",
       "      <th>vwap</th>\n",
       "      <th>volume</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>time_close</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2009-09-29 13:23:42</th>\n",
       "      <td>2009-09-28 09:30:00</td>\n",
       "      <td>50.7900</td>\n",
       "      <td>50.710</td>\n",
       "      <td>51.9600</td>\n",
       "      <td>51.730</td>\n",
       "      <td>51.430024</td>\n",
       "      <td>701459</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2009-10-01 13:44:06</th>\n",
       "      <td>2009-09-29 13:26:31</td>\n",
       "      <td>51.7275</td>\n",
       "      <td>50.070</td>\n",
       "      <td>51.7600</td>\n",
       "      <td>50.270</td>\n",
       "      <td>50.908633</td>\n",
       "      <td>709392</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2009-10-02 14:37:25</th>\n",
       "      <td>2009-10-01 13:45:10</td>\n",
       "      <td>50.2376</td>\n",
       "      <td>49.190</td>\n",
       "      <td>50.3166</td>\n",
       "      <td>49.761</td>\n",
       "      <td>49.836850</td>\n",
       "      <td>724111</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2009-10-07 11:39:00</th>\n",
       "      <td>2009-10-02 14:38:03</td>\n",
       "      <td>49.7635</td>\n",
       "      <td>49.440</td>\n",
       "      <td>51.4700</td>\n",
       "      <td>51.090</td>\n",
       "      <td>50.606475</td>\n",
       "      <td>713127</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2009-10-12 14:24:38</th>\n",
       "      <td>2009-10-07 11:40:15</td>\n",
       "      <td>51.0880</td>\n",
       "      <td>50.980</td>\n",
       "      <td>52.2252</td>\n",
       "      <td>51.990</td>\n",
       "      <td>51.675670</td>\n",
       "      <td>698754</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2024-07-25 11:50:36</th>\n",
       "      <td>2024-07-24 15:51:05</td>\n",
       "      <td>186.6706</td>\n",
       "      <td>186.460</td>\n",
       "      <td>188.5800</td>\n",
       "      <td>188.400</td>\n",
       "      <td>187.387274</td>\n",
       "      <td>186271</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2024-07-25 14:46:14</th>\n",
       "      <td>2024-07-25 11:50:40</td>\n",
       "      <td>188.4000</td>\n",
       "      <td>187.685</td>\n",
       "      <td>189.1286</td>\n",
       "      <td>187.685</td>\n",
       "      <td>188.380972</td>\n",
       "      <td>194394</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2024-07-26 10:11:15</th>\n",
       "      <td>2024-07-25 14:46:35</td>\n",
       "      <td>187.6150</td>\n",
       "      <td>186.980</td>\n",
       "      <td>189.2300</td>\n",
       "      <td>189.020</td>\n",
       "      <td>187.749127</td>\n",
       "      <td>195797</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2024-07-26 15:59:52</th>\n",
       "      <td>2024-07-26 10:12:45</td>\n",
       "      <td>189.0497</td>\n",
       "      <td>188.665</td>\n",
       "      <td>190.0100</td>\n",
       "      <td>189.460</td>\n",
       "      <td>189.348801</td>\n",
       "      <td>183912</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2024-07-26 16:00:00</th>\n",
       "      <td>2024-07-26 16:00:00</td>\n",
       "      <td>189.3900</td>\n",
       "      <td>189.390</td>\n",
       "      <td>189.3900</td>\n",
       "      <td>189.390</td>\n",
       "      <td>189.390000</td>\n",
       "      <td>7734</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>4231 rows × 7 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                              time_open      open      low      high    close  \\\n",
       "time_close                                                                      \n",
       "2009-09-29 13:23:42 2009-09-28 09:30:00   50.7900   50.710   51.9600   51.730   \n",
       "2009-10-01 13:44:06 2009-09-29 13:26:31   51.7275   50.070   51.7600   50.270   \n",
       "2009-10-02 14:37:25 2009-10-01 13:45:10   50.2376   49.190   50.3166   49.761   \n",
       "2009-10-07 11:39:00 2009-10-02 14:38:03   49.7635   49.440   51.4700   51.090   \n",
       "2009-10-12 14:24:38 2009-10-07 11:40:15   51.0880   50.980   52.2252   51.990   \n",
       "...                                 ...       ...      ...       ...      ...   \n",
       "2024-07-25 11:50:36 2024-07-24 15:51:05  186.6706  186.460  188.5800  188.400   \n",
       "2024-07-25 14:46:14 2024-07-25 11:50:40  188.4000  187.685  189.1286  187.685   \n",
       "2024-07-26 10:11:15 2024-07-25 14:46:35  187.6150  186.980  189.2300  189.020   \n",
       "2024-07-26 15:59:52 2024-07-26 10:12:45  189.0497  188.665  190.0100  189.460   \n",
       "2024-07-26 16:00:00 2024-07-26 16:00:00  189.3900  189.390  189.3900  189.390   \n",
       "\n",
       "                           vwap  volume  \n",
       "time_close                               \n",
       "2009-09-29 13:23:42   51.430024  701459  \n",
       "2009-10-01 13:44:06   50.908633  709392  \n",
       "2009-10-02 14:37:25   49.836850  724111  \n",
       "2009-10-07 11:39:00   50.606475  713127  \n",
       "2009-10-12 14:24:38   51.675670  698754  \n",
       "...                         ...     ...  \n",
       "2024-07-25 11:50:36  187.387274  186271  \n",
       "2024-07-25 14:46:14  188.380972  194394  \n",
       "2024-07-26 10:11:15  187.749127  195797  \n",
       "2024-07-26 15:59:52  189.348801  183912  \n",
       "2024-07-26 16:00:00  189.390000    7734  \n",
       "\n",
       "[4231 rows x 7 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# load ../data/variables_ch2.pkl\n",
    "%run ../ch_02/code_ch_02.py\n",
    "\n",
    "path = '../../data/variables_ch2.pkl'\n",
    "import pickle\n",
    "with open(path, 'rb') as f:\n",
    "    bars = pickle.load(f)\n",
    "    bar_time = pickle.load(f)\n",
    "    \n",
    "# df as bars['Dollar'].df_OLHC without 'cusum' column\n",
    "df = bars['Dollar'].df_OLHC.drop(columns=['cusum'])\n",
    "# For the purpose of this example, remove rows where time_close is duplicated\n",
    "# (keep the first row)\n",
    "# Remove rows where time_close is duplicated, keeping the first occurrence\n",
    "df = df.drop_duplicates(subset='time_close', keep='first')\n",
    "# set index as 'time_close'\n",
    "df = df.set_index('time_close')\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n**4.1** In Chapter 3, we denoted as `t1` a pandas series of timestamps where the first barrier was touched, and the index was the timestamp of the observation. This was the output of the `getEvents` function.\\n\\n- **(a)** Compute a `t1` series on dollar bars derived from E-mini S&P 500 futures tick data.\\n  \\n- **(b)** Apply the function `mpNumCoEvents` to compute the number of overlapping outcomes at each point in time.\\n  \\n- **(c)** Plot the time series of the number of concurrent labels on the primary axis, and the time series of exponentially weighted moving standard deviation of returns on the secondary axis.\\n  \\n- **(d)** Produce a scatterplot of the number of concurrent labels (x-axis) and the exponentially weighted moving standard deviation of returns (y-axis). Can you appreciate a relationship?\\n'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "**4.1** In Chapter 3, we denoted as `t1` a pandas series of timestamps where the first barrier was touched, and the index was the timestamp of the observation. This was the output of the `getEvents` function.\n",
    "\n",
    "- **(a)** Compute a `t1` series on dollar bars derived from E-mini S&P 500 futures tick data.\n",
    "  \n",
    "- **(b)** Apply the function `mpNumCoEvents` to compute the number of overlapping outcomes at each point in time.\n",
    "  \n",
    "- **(c)** Plot the time series of the number of concurrent labels on the primary axis, and the time series of exponentially weighted moving standard deviation of returns on the secondary axis.\n",
    "  \n",
    "- **(d)** Produce a scatterplot of the number of concurrent labels (x-axis) and the exponentially weighted moving standard deviation of returns (y-axis). Can you appreciate a relationship?\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **4.1** In Chapter 3, we denoted as `t1` a pandas series of timestamps where the first barrier was touched, and the index was the timestamp of the observation. This was the output of the `getEvents` function.\n",
    "\n",
    "**(a)** Compute a `t1` series on dollar bars derived from E-mini S&P 500 futures tick data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2009-10-02 14:37:25   2009-10-15 12:03:22\n",
       "2009-10-07 11:39:00   2009-10-20 11:12:37\n",
       "2009-10-20 11:12:37   2009-11-02 10:30:21\n",
       "2009-10-27 11:06:15   2009-11-10 10:55:00\n",
       "2009-11-02 10:30:21   2009-11-13 12:12:50\n",
       "                              ...        \n",
       "2024-07-02 14:11:15   2024-07-12 16:00:00\n",
       "2024-07-10 13:21:10   2024-07-22 14:12:45\n",
       "2024-07-11 12:01:59   2024-07-22 14:12:45\n",
       "2024-07-12 16:00:00   2024-07-23 10:51:52\n",
       "2024-07-16 10:07:09   2024-07-26 10:11:15\n",
       "Name: time_close, Length: 1513, dtype: datetime64[ns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Step 1 : get the daily volatility\n",
    "close = df['close']\n",
    "dailyVol = f_ch3.getDailyVol(close, span0=100).dropna()\n",
    "\n",
    "# Step 2 : Detect events (significant points in time where price changes)\n",
    "yt = close.pct_change().dropna()\n",
    "tEvents = f_ch3.getTEvents(yt, h=dailyVol)\n",
    "\n",
    "# Step 3 : Obtain t1 series\n",
    "NUM_DAYS = 10\n",
    "t1 = f_ch3.addVerticalBarrier(tEvents, close, numDays=NUM_DAYS)\n",
    "t1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>t1</th>\n",
       "      <th>trgt</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2009-10-02 14:37:25</th>\n",
       "      <td>2009-10-07 11:39:00</td>\n",
       "      <td>0.012797</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2009-10-07 11:39:00</th>\n",
       "      <td>2009-10-15 12:03:22</td>\n",
       "      <td>0.028053</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2009-10-20 11:12:37</th>\n",
       "      <td>2009-10-27 11:06:15</td>\n",
       "      <td>0.020279</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2009-10-27 11:06:15</th>\n",
       "      <td>2009-11-03 11:21:01</td>\n",
       "      <td>0.020823</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2009-11-02 10:30:21</th>\n",
       "      <td>2009-11-10 10:55:00</td>\n",
       "      <td>0.018378</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2023-11-02 13:19:47</th>\n",
       "      <td>2023-11-03 12:07:30</td>\n",
       "      <td>0.010664</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2023-11-03 12:07:30</th>\n",
       "      <td>2023-11-14 10:00:23</td>\n",
       "      <td>0.011264</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2023-11-03 15:23:46</th>\n",
       "      <td>2023-11-14 10:00:23</td>\n",
       "      <td>0.011404</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2023-11-14 10:00:23</th>\n",
       "      <td>2023-11-20 13:08:22</td>\n",
       "      <td>0.011173</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2023-11-14 15:59:59</th>\n",
       "      <td>2023-11-20 13:08:22</td>\n",
       "      <td>0.011389</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>967 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                     t1      trgt\n",
       "2009-10-02 14:37:25 2009-10-07 11:39:00  0.012797\n",
       "2009-10-07 11:39:00 2009-10-15 12:03:22  0.028053\n",
       "2009-10-20 11:12:37 2009-10-27 11:06:15  0.020279\n",
       "2009-10-27 11:06:15 2009-11-03 11:21:01  0.020823\n",
       "2009-11-02 10:30:21 2009-11-10 10:55:00  0.018378\n",
       "...                                 ...       ...\n",
       "2023-11-02 13:19:47 2023-11-03 12:07:30  0.010664\n",
       "2023-11-03 12:07:30 2023-11-14 10:00:23  0.011264\n",
       "2023-11-03 15:23:46 2023-11-14 10:00:23  0.011404\n",
       "2023-11-14 10:00:23 2023-11-20 13:08:22  0.011173\n",
       "2023-11-14 15:59:59 2023-11-20 13:08:22  0.011389\n",
       "\n",
       "[967 rows x 2 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "# Step 4 : Filter events where target is higher than threshold\n",
    "# Create target series\n",
    "ptsl = [1,1]\n",
    "target=dailyVol\n",
    "# Select minRet\n",
    "minRet = 0.01\n",
    "events = f_ch3.getEvents(close,tEvents,ptsl,target,minRet,numThreads = 1 ,t1=t1)\n",
    "events"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**(b)** Apply the function `mpNumCoEvents` to compute the number of overlapping outcomes at each point in time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# **SNIPPET 4.1** **ESTIMATING THE UNIQUENESS OF A LABEL**\n",
    "\n",
    "def mpNumCoEvents(closeIdx, t1, molecule):\n",
    "    '''\n",
    "    Compute the number of concurrent events per bar.\n",
    "    +molecule[0] is the date of the first event on which the weight will be computed\n",
    "    +molecule[-1] is the date of the last event on which the weight will be computed\n",
    "    Any event that starts before t1[molecule].max() impacts the count.\n",
    "    '''\n",
    "    #1) find events that span the period [molecule[0], molecule[-1]]\n",
    "    t1 = t1.fillna(closeIdx[-1])  # unclosed events still must impact other weights\n",
    "    t1 = t1[t1 >= molecule[0]]  # events that end at or after molecule[0]\n",
    "    t1 = t1.loc[:t1[molecule].max()]  # events that start at or before t1[molecule].max()\n",
    "    \n",
    "    #2) count events spanning a bar\n",
    "    iloc = closeIdx.searchsorted(np.array([t1.index[0], t1.max()]))\n",
    "    count = pd.Series(0, index=closeIdx[iloc[0]:iloc[1] + 1])\n",
    "    for tIn, tOut in t1.items():  # Changed from iteritems() to items()\n",
    "        count.loc[tIn:tOut] += 1\n",
    "    return count.loc[molecule[0]:t1[molecule].max()]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatetimeIndex(['2009-09-29 13:23:42', '2009-10-01 13:44:06',\n",
       "               '2009-10-02 14:37:25', '2009-10-07 11:39:00',\n",
       "               '2009-10-12 14:24:38', '2009-10-15 12:03:22',\n",
       "               '2009-10-20 11:12:37', '2009-10-22 15:42:26',\n",
       "               '2009-10-27 11:06:15', '2009-10-29 11:47:25',\n",
       "               ...\n",
       "               '2024-07-22 14:12:45', '2024-07-23 10:51:52',\n",
       "               '2024-07-23 15:42:59', '2024-07-24 11:32:35',\n",
       "               '2024-07-24 15:50:56', '2024-07-25 11:50:36',\n",
       "               '2024-07-25 14:46:14', '2024-07-26 10:11:15',\n",
       "               '2024-07-26 15:59:52', '2024-07-26 16:00:00'],\n",
       "              dtype='datetime64[ns]', name='time_close', length=4231, freq=None)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "close.index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "time_close\n",
       "2023-11-08 12:47:57    2\n",
       "2023-11-09 12:32:28    2\n",
       "2023-11-10 15:55:48    2\n",
       "2023-11-14 10:00:23    3\n",
       "2023-11-14 15:59:59    2\n",
       "2023-11-15 11:12:01    2\n",
       "2023-11-16 10:23:46    2\n",
       "2023-11-16 15:51:41    2\n",
       "2023-11-17 16:00:00    2\n",
       "2023-11-20 13:08:22    2\n",
       "dtype: int64"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# mpNumCoEvents(close.index, t1, events.index)\n",
    "mpNumCoEvents(closeIdx = close.index, t1 = events['t1'], molecule = events.index)[-10:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "time_close\n",
       "2023-11-15 11:12:01    2\n",
       "2023-11-16 10:23:46    2\n",
       "2023-11-16 15:51:41    2\n",
       "2023-11-17 16:00:00    2\n",
       "2023-11-20 13:08:22    2\n",
       "2023-11-21 09:48:07    2\n",
       "2023-11-21 15:26:36    2\n",
       "2023-11-22 13:08:34    2\n",
       "2023-11-24 10:33:05    2\n",
       "2023-11-27 09:56:25    1\n",
       "dtype: int64"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mpNumCoEvents(closeIdx = close.index, t1 = t1, molecule = events.index)[-10:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **(c)** Plot the time series of the number of concurrent labels on the primary axis, and the time series of exponentially weighted moving standard deviation of returns on the secondary axis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Compute the number of concurrent events\n",
    "numCoEvents = mpNumCoEvents(close.index, t1, events.index)\n",
    "\n",
    "# Step 2: Compute the exponentially weighted moving standard deviation of returns\n",
    "SPAN = 50\n",
    "ewm_std = close.pct_change().ewm(span=SPAN).std()\n",
    "\n",
    "# Step 3: Plot the time series\n",
    "fig, ax1 = plt.subplots(figsize=(14, 7))\n",
    "\n",
    "# Plot the number of concurrent events on the primary axis\n",
    "color = 'tab:blue'\n",
    "ax1.set_xlabel('Date')\n",
    "ax1.set_ylabel('Number of Concurrent Events', color=color)\n",
    "ax1.plot(numCoEvents.index, numCoEvents, color=color, label='Concurrent Events')\n",
    "ax1.tick_params(axis='y', labelcolor=color)\n",
    "\n",
    "# Create a secondary axis to plot the exponentially weighted moving standard deviation of returns\n",
    "ax2 = ax1.twinx()\n",
    "color = 'tab:orange'\n",
    "ax2.set_ylabel('EWM Standard Deviation of Returns', color=color)\n",
    "ax2.plot(ewm_std.index, ewm_std, color=color, label='EWM Std of Returns')\n",
    "ax2.tick_params(axis='y', labelcolor=color)\n",
    "\n",
    "# Add a title and show the plot\n",
    "plt.title('Number of Concurrent Events and EWM Std of Returns')\n",
    "fig.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **(d)** Produce a scatterplot of the number of concurrent labels (x-axis) and the exponentially weighted moving standard deviation of returns (y-axis). Can you appreciate a relationship?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Align the number of concurrent events and the EWM standard deviation of returns\n",
    "aligned_numCoEvents = numCoEvents.reindex(ewm_std.index).dropna()\n",
    "aligned_ewm_std = ewm_std.reindex(aligned_numCoEvents.index).dropna()\n",
    "\n",
    "# Produce the scatterplot\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.scatter(aligned_numCoEvents, aligned_ewm_std)\n",
    "plt.xlabel('Number of Concurrent Labels')\n",
    "plt.ylabel('EWM Standard Deviation of Returns')\n",
    "plt.title('Scatterplot: Concurrent Labels vs. EWM Std of Returns')\n",
    "plt.grid(True)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2 Using the function `mpSampleTW`, compute the average uniqueness of each label. What is the first-order serial correlation, AR(1), of this time series? Is it statistically significant? Why?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the Labeled dataframe, on which we'll add the uniqueness of a label\n",
    "out = f_ch3.getBinsNew(events, close, t1)\n",
    "out = f_ch3.getBins(events, close)\n",
    "out['bin'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "# **SNIPPET 4.2** **ESTIMATING THE AVERAGE UNIQUENESS OF A LABEL**\n",
    "\n",
    "def mpSampleTW(t1, numCoEvents, molecule):\n",
    "    # Derive average uniqueness over the event's lifespan\n",
    "    wght = pd.Series(index=molecule)\n",
    "    for tIn, tOut in t1.loc[wght.index].items():  # Changed from .iteritems() to .items()\n",
    "        wght.loc[tIn] = (1. / numCoEvents.loc[tIn:tOut]).mean()\n",
    "    return wght"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "numCoEvents = f_ch3.mpPandasObj(\n",
    "    mpNumCoEvents,\n",
    "    (\"molecule\", events.index),\n",
    "    numThreads=1,\n",
    "    closeIdx=close.index,\n",
    "    t1=events[\"t1\"],\n",
    ")\n",
    "# numCoEvents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "numCoEvents = numCoEvents.loc[~numCoEvents.index.duplicated(keep=\"last\")]\n",
    "# numCoEvents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "numCoEvents = numCoEvents.reindex(close.index).fillna(0)\n",
    "# numCoEvents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "out[\"tW\"] = f_ch3.mpPandasObj(\n",
    "    mpSampleTW,\n",
    "    (\"molecule\", events.index),\n",
    "    numThreads=1,\n",
    "    t1=events[\"t1\"],\n",
    "    numCoEvents=numCoEvents,\n",
    ")\n",
    "\n",
    "# average uniqueness of a label\n",
    "print(f\"The average uniqueness of a label is: {out['tW'].mean()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### First-order serial correlation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assume `out[\"tW\"]` contains the average uniqueness values computed earlier\n",
    "\n",
    "# Step 1: Compute the AR(1) coefficient\n",
    "ar1_coefficient = out[\"tW\"].autocorr(lag=1)\n",
    "\n",
    "# Step 2: Calculate the t-statistic to test for significance\n",
    "n = len(out[\"tW\"].dropna())  # Number of observations (excluding NaN values)\n",
    "t_statistic = ar1_coefficient * np.sqrt((n - 2) / (1 - ar1_coefficient**2))\n",
    "\n",
    "# Step 3: Calculate the p-value\n",
    "p_value = 2 * (1 - stats.t.cdf(np.abs(t_statistic), df=n - 2))\n",
    "\n",
    "# Output the AR(1) coefficient, t-statistic, and p-value\n",
    "print(f\"AR(1) Coefficient: {ar1_coefficient}\")\n",
    "print(f\"T-statistic: {t_statistic}\")\n",
    "print(f\"P-value: {p_value}\")\n",
    "\n",
    "# Interpretation\n",
    "if p_value < 0.05:\n",
    "    print(\"The AR(1) coefficient is statistically significant.\")\n",
    "else:\n",
    "    print(\"The AR(1) coefficient is not statistically significant.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "**4.3** Fit a random forest to a financial dataset where \\( I^{-1} \\sum_{i=1}^{I} \\tilde{u}_i \\ll 1 \\).\n",
    "\n",
    "- **(a)** What is the mean out-of-bag accuracy?\n",
    "  \n",
    "- **(b)** What is the mean accuracy of k-fold cross-validation (without shuffling) on the same dataset?\n",
    "  \n",
    "- **(c)** Why is out-of-bag accuracy so much higher than cross-validation accuracy? Which one is more correct / less biased? What is the source of this bias?\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **4.3** Fit a random forest to a financial dataset where $I^{-1} \\sum_{i=1}^{I} \\tilde{u}_i \\ll 1$ ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's add features in the dataset to fit a random forest model\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_ = df.copy()\n",
    "\n",
    "# 1. Rolling Volatility (Standard Deviation)\n",
    "df['rolling_vol_10'] = df['close'].rolling(window=10).std()\n",
    "df['rolling_vol_20'] = df['close'].rolling(window=20).std()\n",
    "\n",
    "# 2. Moving Averages\n",
    "df['MA_10'] = df['close'].rolling(window=10).mean()\n",
    "df['MA_20'] = df['close'].rolling(window=20).mean()  # Define MA_20 for Bollinger Bands\n",
    "df['MA_50'] = df['close'].rolling(window=50).mean()\n",
    "\n",
    "# 3. Exponential Moving Averages (EMA)\n",
    "df['EMA_10'] = df['close'].ewm(span=10, adjust=False).mean()\n",
    "df['EMA_50'] = df['close'].ewm(span=50, adjust=False).mean()\n",
    "\n",
    "# 4. Relative Strength Index (RSI)\n",
    "delta = df['close'].diff(1)\n",
    "gain = delta.where(delta > 0, 0)\n",
    "loss = -delta.where(delta < 0, 0)\n",
    "\n",
    "avg_gain = gain.rolling(window=14).mean()\n",
    "avg_loss = loss.rolling(window=14).mean()\n",
    "\n",
    "rs = avg_gain / avg_loss\n",
    "df['RSI'] = 100 - (100 / (1 + rs))\n",
    "\n",
    "# 5. Bollinger Bands\n",
    "df['BB_upper'] = df['MA_20'] + (df['rolling_vol_20'] * 2)\n",
    "df['BB_lower'] = df['MA_20'] - (df['rolling_vol_20'] * 2)\n",
    "\n",
    "# 6. MACD (Moving Average Convergence Divergence)\n",
    "df['EMA_12'] = df['close'].ewm(span=12, adjust=False).mean()\n",
    "df['EMA_26'] = df['close'].ewm(span=26, adjust=False).mean()\n",
    "df['MACD'] = df['EMA_12'] - df['EMA_26']\n",
    "df['Signal_Line'] = df['MACD'].ewm(span=9, adjust=False).mean()\n",
    "\n",
    "# 7. Bar duration as time_close (index) - time_open\n",
    "df['bar_duration'] = df.index.to_series().diff().dt.total_seconds()\n",
    "\n",
    "# Drop any rows with NaN values (due to rolling calculations)\n",
    "df = df.dropna()\n",
    "\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# change index name of 'out' to 'time_close'\n",
    "out.index.name = 'time_close'\n",
    "out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create dataframe 'dataset' which is \"out left join df on index\"\n",
    "dataset = pd.merge(out, df, left_index=True, right_index=True, how='inner')\n",
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save 'dataset' as a pickle file \"dataset_ch4_3.pkl\"\n",
    "path = '../../data/dataset_ch4_3.pkl'\n",
    "with open(path, 'wb') as f:\n",
    "    pickle.dump(dataset, f)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import GridSearchCV, train_test_split\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "from sklearn.preprocessing import MinMaxScaler, StandardScaler\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.impute import SimpleImputer\n",
    "\n",
    "N_ESTIMATORS = 100\n",
    "\n",
    "# Define the features and target\n",
    "X = dataset.drop(columns=[\"bin\", \"ret\", \"time_open\"])  # Dropping 'bin' and 'ret'\n",
    "y = dataset[\"bin\"]  # Target is the 'bin' column\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=RANDOM_STATE, shuffle=False\n",
    ")\n",
    "\n",
    "# Create a column transformer for preprocessing\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        (\n",
    "            \"num\",\n",
    "            Pipeline(\n",
    "                [\n",
    "                    # (\"imputer\", SimpleImputer(strategy=\"median\")),\n",
    "                    (\"scaler\", MinMaxScaler()),\n",
    "                ]\n",
    "            ),\n",
    "            [\n",
    "                \"bar_duration\",\n",
    "                \"close\",\n",
    "                \"vwap\",\n",
    "                \"rolling_vol_10\",\n",
    "                \"rolling_vol_20\",\n",
    "                \"MA_10\",\n",
    "                \"MA_20\",\n",
    "                \"MA_50\",\n",
    "                \"EMA_10\",\n",
    "                \"EMA_50\",\n",
    "                \"RSI\",\n",
    "                \"BB_upper\",\n",
    "                \"BB_lower\",\n",
    "            ],\n",
    "        ),\n",
    "        (\"macd\", Pipeline([(\"scaler\", StandardScaler())]), [\"MACD\"]),\n",
    "    ],\n",
    "    remainder=\"passthrough\",\n",
    ")\n",
    "\n",
    "# Create a pipeline that includes both the preprocessor and the model\n",
    "pipeline = Pipeline(\n",
    "    steps=[\n",
    "        (\"preprocessor\", preprocessor),\n",
    "        (\n",
    "            \"classifier\",\n",
    "            RandomForestClassifier(\n",
    "                n_estimators=N_ESTIMATORS,\n",
    "                random_state=RANDOM_STATE,\n",
    "                bootstrap=True,\n",
    "                oob_score=True,\n",
    "            ),\n",
    "        ),\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CV_search = 20\n",
    "CV = 5\n",
    "\n",
    "# Set up GridSearchCV to optimize for accuracy and include max_features for feature bootstrapping\n",
    "param_grid = {\n",
    "    'classifier__n_estimators': [50, 100, 200],\n",
    "    'classifier__max_depth': [None, 10, 20, 30],\n",
    "    'classifier__min_samples_split': [2, 5, 10],\n",
    "    'classifier__max_features': ['sqrt', 'log2', None]\n",
    "}\n",
    "\n",
    "# Initialize GridSearchCV with accuracy as the scoring metric\n",
    "grid_search = GridSearchCV(pipeline, param_grid, scoring='accuracy', cv=CV, n_jobs=-1)\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "# Get the best estimator from the grid search\n",
    "best_model = grid_search.best_estimator_\n",
    "\n",
    "# Retrieve the mean out-of-bag accuracy\n",
    "oob_accuracy = best_model.named_steps['classifier'].oob_score_\n",
    "\n",
    "# Make predictions on the test data\n",
    "y_pred = best_model.predict(X_test)\n",
    "\n",
    "# Evaluate the model\n",
    "best_params = grid_search.best_params_\n",
    "classification_rep = classification_report(y_test, y_pred)\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "\n",
    "print(f\"Best Parameters: {best_params}\")\n",
    "print(f\"OOB Accuracy: {oob_accuracy}\")\n",
    "print(f\"Test Accuracy: {accuracy}\")\n",
    "print(f\"Classification Report:\\n{classification_rep}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.model_selection import GridSearchCV\n",
    "# from sklearn.metrics import accuracy_score, classification_report\n",
    "# from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "# # Initialize the Random Forest Classifier with bootstrapping for features and enable OOB scoring\n",
    "# N_ESTIMATORS = 100\n",
    "# CV = 10\n",
    "# rf_clf = RandomForestClassifier(n_estimators=N_ESTIMATORS, random_state=RANDOM_STATE, bootstrap=True, oob_score=True)\n",
    "\n",
    "# # Set up GridSearchCV to optimize for accuracy and include max_features for feature bootstrapping\n",
    "# param_grid = {\n",
    "#     'n_estimators': [50, 100, 200],\n",
    "#     'max_depth': [None, 10, 20, 30],\n",
    "#     'min_samples_split': [2, 5, 10],\n",
    "#     'max_features': ['auto', 'sqrt', 'log2']  # Bootstrapping features with different strategies\n",
    "# }\n",
    "\n",
    "# # Initialize GridSearchCV with accuracy as the scoring metric and enable parallelization\n",
    "# grid_search = GridSearchCV(rf_clf, param_grid, scoring='accuracy', cv=CV, n_jobs=-1)  # n_jobs=-1 uses all available cores\n",
    "# grid_search.fit(X_train, y_train)\n",
    "\n",
    "# # Get the best estimator from the grid search\n",
    "# rf_clf = grid_search.best_estimator_\n",
    "\n",
    "# # Retrieve the mean out-of-bag accuracy\n",
    "# oob_accuracy = rf_clf.oob_score_\n",
    "\n",
    "# # Make predictions on the test data\n",
    "# y_pred = rf_clf.predict(X_test)\n",
    "\n",
    "# # Evaluate the model\n",
    "# best_params = grid_search.best_params_\n",
    "# classification_rep = classification_report(y_test, y_pred)\n",
    "# accuracy = accuracy_score(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(classification_rep)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import shap\n",
    "shap.initjs()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract the fitted RandomForestClassifier from the pipeline\n",
    "rf_clf = best_model.named_steps['classifier']\n",
    "\n",
    "# Initialize the SHAP explainer with the fitted RandomForestClassifier\n",
    "explainer = shap.TreeExplainer(rf_clf)\n",
    "\n",
    "# Calculate SHAP values for the test set\n",
    "shap_values = explainer.shap_values(X_test)\n",
    "\n",
    "# Summarize the feature importance\n",
    "shap.summary_plot(shap_values[1], X_test, plot_type=\"bar\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a summary plot\n",
    "shap.summary_plot(shap_values[1], X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize the first prediction's SHAP values\n",
    "shap.force_plot(explainer.expected_value[1], shap_values[1][0], X_test.iloc[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "importances = rf_clf.feature_importances_\n",
    "\n",
    "# Create a plot to visualize the feature importances\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.barh(X_train.columns, importances, color='skyblue')\n",
    "plt.xlabel('Feature Importance')\n",
    "plt.ylabel('Feature')\n",
    "plt.title('Random Forest Feature Importance')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **(a)** What is the mean out-of-bag accuracy?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Mean Out-of-Bag Accuracy: {oob_accuracy:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **(b)** What is the mean accuracy of k-fold cross-validation (without shuffling) on the same dataset?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "# Perform k-fold cross-validation with k=10 (without shuffling)\n",
    "CV = 5\n",
    "cv_scores = cross_val_score(pipeline, X_train, y_train, cv=CV, scoring='accuracy')\n",
    "\n",
    "# Calculate the mean accuracy\n",
    "mean_accuracy = cv_scores.mean()\n",
    "print(f\"Mean Accuracy of k-fold Cross-Validation (Without Shuffling): {mean_accuracy:.4f}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **(c)** Why is out-of-bag accuracy so much higher than cross-validation accuracy? Which one is more correct / less biased? What is the source of this bias?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Explanation of Metrics\n",
    "\n",
    "#### 1. **Out-of-Bag (OOB) Accuracy**\n",
    "- **Definition**: Out-of-bag (OOB) accuracy is a performance measure specific to ensemble methods like Random Forests that use bootstrap aggregating (bagging). In bagging, each tree in the forest is trained on a bootstrap sample, which is a subset of the training data. OOB accuracy is calculated using the instances that were not included in the bootstrap sample for each tree (these are referred to as the \"out-of-bag\" samples).\n",
    "- **Purpose**: OOB accuracy provides an unbiased estimate of the model's performance without requiring a separate validation set. It's calculated during the training process, so it doesn't require any additional data splitting.\n",
    "- **How It Works**: For each data point, only the trees that did not see that data point during training (because it was not in their bootstrap sample) are used to predict its label. The OOB accuracy is the proportion of correct predictions for all the out-of-bag samples.\n",
    "\n",
    "#### 2. **K-Fold Cross-Validation Accuracy**\n",
    "- **Definition**: K-fold cross-validation is a statistical method used to evaluate a model's performance by splitting the data into `k` equally sized folds. The model is trained on `k-1` folds and validated on the remaining fold. This process is repeated `k` times, with each fold being used as the validation set once. The final cross-validation accuracy is the average accuracy across all `k` iterations.\n",
    "- **Purpose**: K-fold cross-validation is used to assess the generalizability of a model. It provides a more robust estimate of model performance by ensuring that every observation is used for both training and validation.\n",
    "- **How It Works**: Each time the model is trained on `k-1` folds and tested on the remaining fold, this gives an estimate of how the model performs on unseen data. After repeating this process `k` times, the average accuracy is computed.\n",
    "\n",
    "### Comparison and Answer to the Question\n",
    "\n",
    "#### (c) Why is out-of-bag accuracy so much higher than cross-validation accuracy? Which one is more correct / less biased? What is the source of this bias?\n",
    "\n",
    "**Observation:**\n",
    "- You observed that the mean out-of-bag accuracy is higher than the mean accuracy from k-fold cross-validation.\n",
    "\n",
    "**Reasons for the Difference:**\n",
    "\n",
    "1. **Training Data Differences:**\n",
    "   - **OOB Accuracy**: Each tree in the Random Forest is trained on a slightly different subset of the training data due to bootstrapping. The OOB accuracy is calculated on the data that was not seen by the individual trees during their training. However, since Random Forests are designed to perform well on out-of-bag samples by aggregating the predictions of multiple trees, OOB accuracy often reflects an optimistic estimate of performance.\n",
    "   - **Cross-Validation Accuracy**: In contrast, during k-fold cross-validation, the model is tested on a fold of data that was completely withheld during training. This process better simulates how the model will perform on entirely unseen data, providing a more realistic estimate of the model's generalization ability.\n",
    "\n",
    "2. **Overfitting on OOB Samples:**\n",
    "   - **OOB Accuracy**: Since the OOB accuracy is calculated using only the subset of trees that did not see the data point during training, it might be slightly optimistic. Even though each tree hasn't seen the OOB samples, the ensemble as a whole has a stronger tendency to perform well on these samples due to the aggregating effect of the forest.\n",
    "   - **Cross-Validation Accuracy**: K-fold cross-validation, especially when not shuffled, may capture nuances in the data that could lead to overfitting, depending on the data's structure. If the dataset has any inherent order or if the model is overfitting the training data, the cross-validation score could be lower.\n",
    "\n",
    "3. **Bias and Correctness:**\n",
    "   - **OOB Accuracy**: While OOB accuracy is useful and often close to the true performance of the model, it can be slightly optimistic due to the reasons mentioned above.\n",
    "   - **Cross-Validation Accuracy**: Cross-validation accuracy is generally considered to be a more robust and less biased estimate of the model's true generalization performance, especially when done without shuffling. It provides a comprehensive evaluation by testing the model on completely unseen data during each fold.\n",
    "\n",
    "**Conclusion:**\n",
    "- **Which One Is More Correct/Less Biased?**: In most cases, k-fold cross-validation accuracy is considered more correct and less biased, particularly when you are concerned with how well the model will generalize to entirely new, unseen data. Cross-validation evaluates the model's performance across multiple splits of the data, making it a reliable measure of generalization.\n",
    "- **Source of Bias**: The bias in OOB accuracy arises because the OOB samples are still part of the overall training data, even though they aren't used by the individual trees. This can lead to a slight overestimation of the model's performance.\n",
    "\n",
    "In summary, while OOB accuracy provides a quick and useful estimate of model performance, k-fold cross-validation without shuffling generally gives a more accurate and less biased assessment of the model's generalization ability. The difference you observed, where OOB accuracy is higher than cross-validation accuracy, likely stems from the optimistic nature of OOB evaluation and the more rigorous testing applied during cross-validation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "# **SNIPPET 4.10** **DETERMINATION OF SAMPLE WEIGHT BY ABSOLUTE RETURN ATTRIBUTION**\n",
    "def mpSampleW(t1, numCoEvents, close, molecule):\n",
    "    # Derive sample weight by return attribution\n",
    "    ret = np.log(close).diff()  # log-returns, so that they are additive\n",
    "    wght = pd.Series(index=molecule)\n",
    "    for tIn, tOut in t1.loc[wght.index].items():  # Changed from iteritems() to items()\n",
    "        wght.loc[tIn] = (ret.loc[tIn:tOut] / numCoEvents.loc[tIn:tOut]).sum()\n",
    "    return wght.abs()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "out[\"w\"] = f_ch3.mpPandasObj(\n",
    "    mpSampleW,\n",
    "    (\"molecule\", events.index),\n",
    "    numThreads=1,\n",
    "    t1=events[\"t1\"],\n",
    "    numCoEvents=numCoEvents,\n",
    "    close=close,\n",
    ")\n",
    "\n",
    "out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "out['w'] *= out.shape[0] / out['w'].sum()\n",
    "\n",
    "out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "out['tW']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **4.4** Modify the code in Section 4.7 to apply an exponential time-decay factor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# **SNIPPET 4.11** **IMPLEMENTATION OF TIME-DECAY FACTORS**\n",
    "\n",
    "\n",
    "def getTimeDecay(tW, clfLastW=1.0):\n",
    "    # apply piecewise-linear decay to observed uniqueness (tW)\n",
    "    # newest observation gets weight=1, oldest observation gets weight=clfLastW\n",
    "    clfW = tW.sort_index().cumsum()\n",
    "    if clfLastW >= 0:\n",
    "        slope = (1.0 - clfLastW) / clfW.iloc[-1]\n",
    "    else:\n",
    "        slope = 1.0 / ((clfLastW + 1) * clfW.iloc[-1])\n",
    "    const = 1.0 - slope * clfW.iloc[-1]\n",
    "    clfW = const + slope * clfW\n",
    "    clfW[clfW < 0] = 0\n",
    "    print(const, slope)\n",
    "    return clfW\n",
    "\n",
    "\n",
    "getTimeDecay(out[\"tW\"], clfLastW=0.8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def getExpTimeDecay(tW, clfLastW=1.0):\n",
    "    # apply piecewise-linear decay to observed uniqueness (tW)\n",
    "    # newest observation gets weight=1, oldest observation gets weight=clfLastW\n",
    "    clfW = tW.sort_index().cumsum()\n",
    "    if clfLastW > 0:\n",
    "        slope = (-1.0/clfW.iloc[-1])*np.log(clfLastW)\n",
    "    else:\n",
    "        raise ValueError('clfLastW must be greater than 0')\n",
    "    const = - slope * clfW.iloc[-1]\n",
    "    clfW = np.exp(const + slope * clfW)\n",
    "    clfW[clfW < 0] = 0\n",
    "    print(const, slope)\n",
    "    return clfW\n",
    "\n",
    "\n",
    "getExpTimeDecay(out[\"tW\"], clfLastW=0.8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "out['tW']"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "financial_math",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
