{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The watermark extension is already loaded. To reload it, use:\n",
      "  %reload_ext watermark\n",
      "Last updated: 2024-09-06T14:14:06.635145-04:00\n",
      "\n",
      "Python implementation: CPython\n",
      "Python version       : 3.8.19\n",
      "IPython version      : 8.12.2\n",
      "\n",
      "Compiler    : Clang 16.0.6 \n",
      "OS          : Darwin\n",
      "Release     : 23.6.0\n",
      "Machine     : arm64\n",
      "Processor   : arm\n",
      "CPU cores   : 8\n",
      "Architecture: 64bit\n",
      "\n",
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/j0/8v9qcjfx15g5ftmsy5n0qrq80000gn/T/ipykernel_96299/2173331131.py:46: MatplotlibDeprecationWarning: The seaborn styles shipped by Matplotlib are deprecated since 3.6, as they no longer correspond to the styles shipped by seaborn. However, they will remain available as 'seaborn-v0_8-<style>'. Alternatively, directly use the seaborn API instead.\n",
      "  plt.style.use('seaborn-talk')\n"
     ]
    }
   ],
   "source": [
    "%load_ext watermark\n",
    "%watermark\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "\n",
    "# import standard libs\n",
    "from IPython.display import display\n",
    "from IPython.core.debugger import set_trace as bp\n",
    "from pathlib import PurePath, Path\n",
    "import sys\n",
    "import time\n",
    "from collections import OrderedDict as od\n",
    "import re\n",
    "import os\n",
    "import json\n",
    "import datetime\n",
    "import pickle\n",
    "\n",
    "\n",
    "# import python scientific stack\n",
    "import pandas as pd\n",
    "import pandas_datareader.data as web\n",
    "pd.set_option('display.max_rows', 10)\n",
    "from dask import dataframe as dd\n",
    "from dask.diagnostics import ProgressBar\n",
    "from multiprocessing import cpu_count\n",
    "pbar = ProgressBar()\n",
    "pbar.register()\n",
    "import numpy as np\n",
    "import scipy.stats as stats\n",
    "import statsmodels.api as sm\n",
    "from numba import jit\n",
    "import math\n",
    "# import ffn\n",
    "\n",
    "\n",
    "# import visual tools\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.gridspec as gridspec\n",
    "%matplotlib inline\n",
    "import seaborn as sns\n",
    "\n",
    "plt.style.use('seaborn-talk')\n",
    "plt.style.use('bmh')\n",
    "#plt.rcParams['font.family'] = 'DejaVu Sans Mono'\n",
    "plt.rcParams['font.size'] = 9.5\n",
    "plt.rcParams['font.weight'] = 'medium'\n",
    "plt.rcParams['figure.figsize'] = 10,7\n",
    "blue, green, red, purple, gold, teal = sns.color_palette('colorblind', 6)\n",
    "\n",
    "RANDOM_STATE = 777\n",
    "\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Project root added to sys.path: /Users/paulkelendji/Desktop/GitHub_paul/ML-Asset_Management\n",
      "Config path added to sys.path: /Users/paulkelendji/Desktop/GitHub_paul/ML-Asset_Management/config\n",
      "Current sys.path: ['/Users/paulkelendji/miniconda3/envs/financial_math/lib/python38.zip', '/Users/paulkelendji/miniconda3/envs/financial_math/lib/python3.8', '/Users/paulkelendji/miniconda3/envs/financial_math/lib/python3.8/lib-dynload', '', '/Users/paulkelendji/miniconda3/envs/financial_math/lib/python3.8/site-packages', '/Users/paulkelendji/miniconda3/envs/financial_math/lib/python3.8/site-packages/setuptools/_vendor', '/Users/paulkelendji/Desktop/GitHub_paul/ML-Asset_Management', '/Users/paulkelendji/Desktop/GitHub_paul/ML-Asset_Management/config', '/Users/paulkelendji/Desktop/GitHub_paul/ML-Asset_Management', '/Users/paulkelendji/Desktop/GitHub_paul/ML-Asset_Management/config', '/Users/paulkelendji/Desktop/GitHub_paul', '/Users/paulkelendji/Desktop/GitHub_paul/ML-Asset_Management', '/Users/paulkelendji/Desktop/GitHub_paul/ML-Asset_Management/config']\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "# Run the setup script\n",
    "%run ../../config/setup_project.py\n",
    "\n",
    "# Call the function to set up the project path\n",
    "setup_project_path()\n",
    "\n",
    "# Now you can import your modules\n",
    "from src.utils import helper as h_\n",
    "import src.ch_02.code_ch_02 as f_ch2\n",
    "import src.ch_03.code_ch_03 as f_ch3\n",
    "import src.ch_04.code_ch_04 as f_ch4\n",
    "import src.ch_05.code_ch_05 as f_ch5\n",
    "import src.ch_06.code_ch_06 as f_ch6\n",
    "import src.ch_08.code_ch_08 as f_ch8\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load ../data/variables_ch2.pkl\n",
    "%run ../ch_02/code_ch_02.py\n",
    "\n",
    "path = '../../data/variables_ch2.pkl'\n",
    "import pickle\n",
    "with open(path, 'rb') as f:\n",
    "    bars = pickle.load(f)\n",
    "    bar_time = pickle.load(f)\n",
    "    \n",
    "# df as bars['Dollar'].df_OLHC without 'cusum' column\n",
    "df = bars['Dollar'].df_OLHC.drop(columns=['cusum'])\n",
    "# For the purpose of this example, remove rows where time_close is duplicated\n",
    "# (keep the first row)\n",
    "# Remove rows where time_close is duplicated, keeping the first occurrence\n",
    "df = df.drop_duplicates(subset='time_close', keep='first')\n",
    "# set index as 'time_close'\n",
    "df = df.set_index('time_close')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# CLOSE PRICE AND DAILY VOLATILITY\n",
    "# Step 1 : get the daily volatility\n",
    "close = df['close']\n",
    "dailyVol = f_ch3.getDailyVol(close, span0=100).dropna()\n",
    "\n",
    "# from series to df\n",
    "close = pd.DataFrame(close)\n",
    "dailyVol = pd.DataFrame(dailyVol)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6.1 Why is bagging based on random sampling with replacement? Would bagging still reduce a forecast’s variance if sampling were without replacement?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Bagging is effective when models are trained on samples that are as independant/\n",
    "as possible. If we sample without replacement, each sample will depend on the/\n",
    "the previous one, which will make the models more correlated and reduce the/\n",
    "effectiveness of bagging."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6.2 Suppose that your training set is based on highly overlap labels (i.e., with low uniqueness, as defined in Chapter 4).\n",
    "- (a) Does this make bagging prone to overfitting, or just ineffective? Why?\n",
    "- (b) Is out-of-bag accuracy generally reliable in financial applications? Why?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(a) Id samples are not IID at all, the correlation between samples, and thus\\\n",
    "the models will be high, and bagging won't reduce de variace of the prediction,\\\n",
    "therefore innefective.\n",
    "\n",
    "(b) Due to redundancy, the sampling with replacement will create training set\\\n",
    "samples that are very similar to out-of-bag samples. Therefore the out-of-bag\\\n",
    "accuracy is inflated and not reliable.\n",
    "\n",
    "### 6.3.3 Observation Redundancy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6.3 Build an ensemble of estimators, where the base estimator is a decision tree.\n",
    "### (a) How is this ensemble different from an RF?\n",
    "### (b) Using sklearn, produce a bagging classifier that behaves like an RF. What parameters did you have to set up, and how?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>I_0</th>\n",
       "      <th>I_1</th>\n",
       "      <th>I_2</th>\n",
       "      <th>R_0</th>\n",
       "      <th>R_1</th>\n",
       "      <th>R_2</th>\n",
       "      <th>R_3</th>\n",
       "      <th>R_4</th>\n",
       "      <th>R_5</th>\n",
       "      <th>R_6</th>\n",
       "      <th>...</th>\n",
       "      <th>R_27</th>\n",
       "      <th>R_28</th>\n",
       "      <th>R_29</th>\n",
       "      <th>N_0</th>\n",
       "      <th>N_1</th>\n",
       "      <th>N_2</th>\n",
       "      <th>N_3</th>\n",
       "      <th>N_4</th>\n",
       "      <th>N_5</th>\n",
       "      <th>N_6</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1986-05-12 15:57:13.403201</th>\n",
       "      <td>1.748267</td>\n",
       "      <td>2.690969</td>\n",
       "      <td>-0.762359</td>\n",
       "      <td>-1.793042</td>\n",
       "      <td>-0.257715</td>\n",
       "      <td>-2.555273</td>\n",
       "      <td>-1.771813</td>\n",
       "      <td>0.354711</td>\n",
       "      <td>0.950059</td>\n",
       "      <td>0.420574</td>\n",
       "      <td>...</td>\n",
       "      <td>1.270633</td>\n",
       "      <td>-3.875453</td>\n",
       "      <td>-0.313406</td>\n",
       "      <td>-1.101311</td>\n",
       "      <td>0.633439</td>\n",
       "      <td>0.492835</td>\n",
       "      <td>0.711580</td>\n",
       "      <td>-0.752805</td>\n",
       "      <td>3.333879</td>\n",
       "      <td>-1.413558</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1986-05-13 15:57:13.403201</th>\n",
       "      <td>2.089446</td>\n",
       "      <td>2.897283</td>\n",
       "      <td>-0.347177</td>\n",
       "      <td>-2.379418</td>\n",
       "      <td>0.023721</td>\n",
       "      <td>-3.089672</td>\n",
       "      <td>-2.315702</td>\n",
       "      <td>0.767668</td>\n",
       "      <td>0.702689</td>\n",
       "      <td>0.583657</td>\n",
       "      <td>...</td>\n",
       "      <td>1.694423</td>\n",
       "      <td>-4.426854</td>\n",
       "      <td>-0.595684</td>\n",
       "      <td>-0.733146</td>\n",
       "      <td>1.760190</td>\n",
       "      <td>-0.265049</td>\n",
       "      <td>0.039147</td>\n",
       "      <td>0.423577</td>\n",
       "      <td>1.603988</td>\n",
       "      <td>-0.852260</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1986-05-14 15:57:13.403201</th>\n",
       "      <td>1.359057</td>\n",
       "      <td>1.124433</td>\n",
       "      <td>-0.449752</td>\n",
       "      <td>-0.871561</td>\n",
       "      <td>-0.488470</td>\n",
       "      <td>-1.522993</td>\n",
       "      <td>-0.588815</td>\n",
       "      <td>0.096156</td>\n",
       "      <td>-0.011085</td>\n",
       "      <td>0.490655</td>\n",
       "      <td>...</td>\n",
       "      <td>0.283697</td>\n",
       "      <td>-2.085859</td>\n",
       "      <td>0.180009</td>\n",
       "      <td>-0.427140</td>\n",
       "      <td>-1.157007</td>\n",
       "      <td>0.853710</td>\n",
       "      <td>0.540003</td>\n",
       "      <td>0.547669</td>\n",
       "      <td>0.944567</td>\n",
       "      <td>-0.732269</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1986-05-15 15:57:13.403201</th>\n",
       "      <td>0.942504</td>\n",
       "      <td>-0.076773</td>\n",
       "      <td>0.246008</td>\n",
       "      <td>-0.483019</td>\n",
       "      <td>-0.237725</td>\n",
       "      <td>-0.803192</td>\n",
       "      <td>-0.092439</td>\n",
       "      <td>0.243977</td>\n",
       "      <td>-0.826416</td>\n",
       "      <td>0.507796</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.086203</td>\n",
       "      <td>-0.709872</td>\n",
       "      <td>0.198837</td>\n",
       "      <td>1.181418</td>\n",
       "      <td>-0.596872</td>\n",
       "      <td>0.109188</td>\n",
       "      <td>0.289190</td>\n",
       "      <td>1.054555</td>\n",
       "      <td>-1.120703</td>\n",
       "      <td>-0.576455</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1986-05-16 15:57:13.403201</th>\n",
       "      <td>1.076313</td>\n",
       "      <td>1.796803</td>\n",
       "      <td>-1.741340</td>\n",
       "      <td>-0.190357</td>\n",
       "      <td>-1.149907</td>\n",
       "      <td>-1.184029</td>\n",
       "      <td>-0.096258</td>\n",
       "      <td>-0.731054</td>\n",
       "      <td>1.143103</td>\n",
       "      <td>0.169718</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.103153</td>\n",
       "      <td>-2.276741</td>\n",
       "      <td>0.619001</td>\n",
       "      <td>-0.012835</td>\n",
       "      <td>0.525344</td>\n",
       "      <td>-1.820744</td>\n",
       "      <td>0.484778</td>\n",
       "      <td>-0.175199</td>\n",
       "      <td>-1.932807</td>\n",
       "      <td>0.585696</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 40 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                 I_0       I_1       I_2       R_0       R_1  \\\n",
       "1986-05-12 15:57:13.403201  1.748267  2.690969 -0.762359 -1.793042 -0.257715   \n",
       "1986-05-13 15:57:13.403201  2.089446  2.897283 -0.347177 -2.379418  0.023721   \n",
       "1986-05-14 15:57:13.403201  1.359057  1.124433 -0.449752 -0.871561 -0.488470   \n",
       "1986-05-15 15:57:13.403201  0.942504 -0.076773  0.246008 -0.483019 -0.237725   \n",
       "1986-05-16 15:57:13.403201  1.076313  1.796803 -1.741340 -0.190357 -1.149907   \n",
       "\n",
       "                                 R_2       R_3       R_4       R_5       R_6  \\\n",
       "1986-05-12 15:57:13.403201 -2.555273 -1.771813  0.354711  0.950059  0.420574   \n",
       "1986-05-13 15:57:13.403201 -3.089672 -2.315702  0.767668  0.702689  0.583657   \n",
       "1986-05-14 15:57:13.403201 -1.522993 -0.588815  0.096156 -0.011085  0.490655   \n",
       "1986-05-15 15:57:13.403201 -0.803192 -0.092439  0.243977 -0.826416  0.507796   \n",
       "1986-05-16 15:57:13.403201 -1.184029 -0.096258 -0.731054  1.143103  0.169718   \n",
       "\n",
       "                            ...      R_27      R_28      R_29       N_0  \\\n",
       "1986-05-12 15:57:13.403201  ...  1.270633 -3.875453 -0.313406 -1.101311   \n",
       "1986-05-13 15:57:13.403201  ...  1.694423 -4.426854 -0.595684 -0.733146   \n",
       "1986-05-14 15:57:13.403201  ...  0.283697 -2.085859  0.180009 -0.427140   \n",
       "1986-05-15 15:57:13.403201  ... -0.086203 -0.709872  0.198837  1.181418   \n",
       "1986-05-16 15:57:13.403201  ... -0.103153 -2.276741  0.619001 -0.012835   \n",
       "\n",
       "                                 N_1       N_2       N_3       N_4       N_5  \\\n",
       "1986-05-12 15:57:13.403201  0.633439  0.492835  0.711580 -0.752805  3.333879   \n",
       "1986-05-13 15:57:13.403201  1.760190 -0.265049  0.039147  0.423577  1.603988   \n",
       "1986-05-14 15:57:13.403201 -1.157007  0.853710  0.540003  0.547669  0.944567   \n",
       "1986-05-15 15:57:13.403201 -0.596872  0.109188  0.289190  1.054555 -1.120703   \n",
       "1986-05-16 15:57:13.403201  0.525344 -1.820744  0.484778 -0.175199 -1.932807   \n",
       "\n",
       "                                 N_6  \n",
       "1986-05-12 15:57:13.403201 -1.413558  \n",
       "1986-05-13 15:57:13.403201 -0.852260  \n",
       "1986-05-14 15:57:13.403201 -0.732269  \n",
       "1986-05-15 15:57:13.403201 -0.576455  \n",
       "1986-05-16 15:57:13.403201  0.585696  \n",
       "\n",
       "[5 rows x 40 columns]"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Dataset\n",
    "\n",
    "data, cont = f_ch8.getTestData(n_features=40, n_informative=3, n_redundant=30, n_samples=10000)\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "SNIPPET 6.2 THREE WAYS OF SETTING UP AN RF\n",
    "```python\n",
    "clf0=RandomForestClassiﬁer(n_estimators=1000,class_weight='balanced_subsample',\n",
    "criterion='entropy')\n",
    "clf1=DecisionTreeClassiﬁer(criterion='entropy',max_features='auto',\n",
    "class_weight='balanced')\n",
    "clf1=BaggingClassiﬁer(base_estimator=clf1,n_estimators=1000,max_samples=avgU)\n",
    "clf2=RandomForestClassiﬁer(n_estimators=1,criterion='entropy',bootstrap=False,\n",
    "class_weight='balanced_subsample')\n",
    "clf2=BaggingClassiﬁer(base_estimator=clf2,n_estimators=1000,max_samples=avgU,\n",
    "max_features=1.)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's break down how to approach each part of this question.\n",
    "\n",
    "### (a) How is this ensemble different from an RF?\n",
    "\n",
    "The main difference between a general ensemble of decision trees and a Random Forest (RF) lies in the **feature selection** process:\n",
    "\n",
    "- **Ensemble of Decision Trees with Bagging**: In this case, bagging is applied to generate bootstrap samples (random subsets of the data with replacement). A decision tree is trained on each of these subsets, and the final prediction is based on the aggregation (e.g., majority vote for classification) of all the individual trees’ predictions. **However, each node of a decision tree in this ensemble will have access to all features** when deciding on splits, which can lead to more correlated trees.\n",
    "\n",
    "- **Random Forest**: In addition to bagging, Random Forest adds a second level of randomness: at each split in a decision tree, only a random subset of the features is considered for finding the best split. This forces trees to be less correlated by ensuring that not all trees focus on the same strong predictors.\n",
    "\n",
    "**Key Difference**: Random Forest reduces the correlation between trees by limiting the number of features available at each node split, making the ensemble less prone to overfitting and better at generalization than a basic ensemble of decision trees.\n",
    "\n",
    "### (b) Using `sklearn`, produce a bagging classifier that behaves like an RF. What parameters did you have to set up, and how?\n",
    "\n",
    "To make a bagging classifier behave like a Random Forest, you need to:\n",
    "\n",
    "1. Use a **DecisionTreeClassifier** as the base estimator.\n",
    "2. Add randomness by limiting the number of features considered at each node split (`max_features`).\n",
    "3. Set up **bagging** using `BaggingClassifier`, which creates bootstrapped samples.\n",
    "\n",
    "Here’s the code to set up a `BaggingClassifier` that mimics Random Forest behavior:\n",
    "\n",
    "```python\n",
    "from sklearn.ensemble import BaggingClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.datasets import make_classification\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Generate synthetic dataset\n",
    "X, y = make_classification(n_samples=1000, n_features=20, random_state=42)\n",
    "\n",
    "# Split into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "\n",
    "# Decision tree with feature subsampling to mimic RF\n",
    "base_estimator = DecisionTreeClassifier(max_features='sqrt', criterion='entropy')\n",
    "\n",
    "# Bagging classifier with Decision Tree as base estimator\n",
    "bagging_clf = BaggingClassifier(\n",
    "    base_estimator=base_estimator,\n",
    "    n_estimators=100,  # Number of trees\n",
    "    max_samples=1.0,   # Use bootstrapped samples\n",
    "    bootstrap=True,    # Sampling with replacement\n",
    "    n_jobs=-1,         # Parallel processing\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "# Train the classifier\n",
    "bagging_clf.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions and evaluate\n",
    "y_pred = bagging_clf.predict(X_test)\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(f'Bagging Classifier Accuracy: {accuracy:.4f}')\n",
    "```\n",
    "\n",
    "### Parameters to set up:\n",
    "1. **`max_features='sqrt'`**: This is crucial because it limits the number of features considered at each split, which mimics Random Forest's behavior.\n",
    "2. **`n_estimators`**: Number of trees in the ensemble, typically large (100+).\n",
    "3. **`bootstrap=True`**: Ensures sampling with replacement, like in Random Forest.\n",
    "4. **`max_samples=1.0`**: Use all available samples for bootstrapping (same as Random Forest).\n",
    "5. **`criterion='entropy'`**: Using entropy as the criterion for node splits, though this is not mandatory, as `gini` is also common.\n",
    "\n",
    "This setup ensures that the bagging classifier behaves like a Random Forest. Let me know if you need further clarification!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Answering (b) myself"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Libraries\n",
    "from sklearn.ensemble import BaggingClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.datasets import make_classification\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Dataset\n",
    "X, Y = f_ch8.getTestData(n_features=40, n_informative=3, n_redundant=30, n_samples=10000)\n",
    "Y = Y[\"bin\"]\n",
    "\n",
    "# usual RF\n",
    "clf0 = RandomForestClassifier(\n",
    "    n_estimators=1_000,  # 1_000 trees\n",
    "    class_weight=\"balanced_subsample\",  # prevent minority class from being ignored\n",
    "    criterion=\"entropy\"  # information gain\n",
    ")\n",
    "\n",
    "# Ensemble of estimators with base estimator as a decision tree\n",
    "clf1 = DecisionTreeClassifier(\n",
    "    criterion=\"entropy\",  # information gain\n",
    "    max_features=\"sqrt\",  # sqrt(n_features) to force diversity among trees\n",
    "    class_weight=\"balanced\"  # prevent minority class from being ignored\n",
    ")\n",
    "clf1 = BaggingClassifier(\n",
    "    estimator=clf1,  # base estimator\n",
    "    n_estimators=1_000,  # 1_000 trees\n",
    "    max_samples=0.6,  # average uniqueness\n",
    "    max_features=1.0  # all features for bagging\n",
    ")\n",
    "\n",
    "# Bagging classifier on RF where max_samples is set to average uniqueness\n",
    "clf2 = RandomForestClassifier(\n",
    "    n_estimators=1,  # 1 tree\n",
    "    criterion=\"entropy\",  # information gain\n",
    "    bootstrap=False,  # no bootstrap\n",
    "    class_weight=\"balanced_subsample\"  # prevent minority class from being ignored\n",
    ")\n",
    "\n",
    "clf2 = BaggingClassifier(\n",
    "    estimator=clf2,  # base estimator\n",
    "    n_estimators=1_000,  # 1_000 trees\n",
    "    max_samples=0.6,  # average uniqueness\n",
    "    max_features=1.0  # all features for bagging\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RandomForestClassifier 0.9624\n",
      "BaggingClassifier 0.9616\n",
      "BaggingClassifier 0.9616\n"
     ]
    }
   ],
   "source": [
    "classifiers = [clf0, clf1, clf2]\n",
    "\n",
    "# split data\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, Y, test_size=0.5, random_state=42)\n",
    "\n",
    "# fit and predict\n",
    "\n",
    "for clf in classifiers:\n",
    "    clf.fit(X_train, y_train)\n",
    "    y_pred = clf.predict(X_test)\n",
    "    print(clf.__class__.__name__,\n",
    "            accuracy_score(y_test, y_pred))\n",
    "    \n",
    "# The ensemble of estimators with base estimator as a decision tree is different from a RF in that the former uses a single decision tree as the base estimator, while the latter uses a forest of decision trees as the base estimator. The ensemble of estimators with base estimator as a decision tree is more similar to a bagging classifier with a decision tree as the base estimator, as the base estimator is a single decision tree in both cases. The ensemble of estimators with base estimator as a decision tree is also more similar to a bagging classifier with a RF as the base estimator, as the base estimator is a single decision tree in both cases.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "financial_math",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
