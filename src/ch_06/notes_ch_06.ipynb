{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Last updated: 2024-09-06T14:12:31.617466-04:00\n",
      "\n",
      "Python implementation: CPython\n",
      "Python version       : 3.8.19\n",
      "IPython version      : 8.12.2\n",
      "\n",
      "Compiler    : Clang 16.0.6 \n",
      "OS          : Darwin\n",
      "Release     : 23.6.0\n",
      "Machine     : arm64\n",
      "Processor   : arm\n",
      "CPU cores   : 8\n",
      "Architecture: 64bit\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/j0/8v9qcjfx15g5ftmsy5n0qrq80000gn/T/ipykernel_94824/2173331131.py:46: MatplotlibDeprecationWarning: The seaborn styles shipped by Matplotlib are deprecated since 3.6, as they no longer correspond to the styles shipped by seaborn. However, they will remain available as 'seaborn-v0_8-<style>'. Alternatively, directly use the seaborn API instead.\n",
      "  plt.style.use('seaborn-talk')\n"
     ]
    }
   ],
   "source": [
    "%load_ext watermark\n",
    "%watermark\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "\n",
    "# import standard libs\n",
    "from IPython.display import display\n",
    "from IPython.core.debugger import set_trace as bp\n",
    "from pathlib import PurePath, Path\n",
    "import sys\n",
    "import time\n",
    "from collections import OrderedDict as od\n",
    "import re\n",
    "import os\n",
    "import json\n",
    "import datetime\n",
    "import pickle\n",
    "\n",
    "\n",
    "# import python scientific stack\n",
    "import pandas as pd\n",
    "import pandas_datareader.data as web\n",
    "pd.set_option('display.max_rows', 10)\n",
    "from dask import dataframe as dd\n",
    "from dask.diagnostics import ProgressBar\n",
    "from multiprocessing import cpu_count\n",
    "pbar = ProgressBar()\n",
    "pbar.register()\n",
    "import numpy as np\n",
    "import scipy.stats as stats\n",
    "import statsmodels.api as sm\n",
    "from numba import jit\n",
    "import math\n",
    "# import ffn\n",
    "\n",
    "\n",
    "# import visual tools\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.gridspec as gridspec\n",
    "%matplotlib inline\n",
    "import seaborn as sns\n",
    "\n",
    "plt.style.use('seaborn-talk')\n",
    "plt.style.use('bmh')\n",
    "#plt.rcParams['font.family'] = 'DejaVu Sans Mono'\n",
    "plt.rcParams['font.size'] = 9.5\n",
    "plt.rcParams['font.weight'] = 'medium'\n",
    "plt.rcParams['figure.figsize'] = 10,7\n",
    "blue, green, red, purple, gold, teal = sns.color_palette('colorblind', 6)\n",
    "\n",
    "RANDOM_STATE = 777\n",
    "\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Project root added to sys.path: /Users/paulkelendji/Desktop/GitHub_paul/ML-Asset_Management\n",
      "Config path added to sys.path: /Users/paulkelendji/Desktop/GitHub_paul/ML-Asset_Management/config\n",
      "Current sys.path: ['/Users/paulkelendji/miniconda3/envs/financial_math/lib/python38.zip', '/Users/paulkelendji/miniconda3/envs/financial_math/lib/python3.8', '/Users/paulkelendji/miniconda3/envs/financial_math/lib/python3.8/lib-dynload', '', '/Users/paulkelendji/miniconda3/envs/financial_math/lib/python3.8/site-packages', '/Users/paulkelendji/miniconda3/envs/financial_math/lib/python3.8/site-packages/setuptools/_vendor', '/Users/paulkelendji/Desktop/GitHub_paul/ML-Asset_Management', '/Users/paulkelendji/Desktop/GitHub_paul/ML-Asset_Management/config']\n"
     ]
    },
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'utils'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 10\u001b[0m\n\u001b[1;32m      7\u001b[0m setup_project_path()\n\u001b[1;32m      9\u001b[0m \u001b[38;5;66;03m# Now you can import your modules\u001b[39;00m\n\u001b[0;32m---> 10\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mutils\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m helper \u001b[38;5;28;01mas\u001b[39;00m h_\n\u001b[1;32m     11\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mch_02\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcode_ch_02\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mf_ch2\u001b[39;00m\n\u001b[1;32m     12\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mch_03\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcode_ch_03\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mf_ch3\u001b[39;00m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'utils'"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "# Run the setup script\n",
    "%run ../../config/setup_project.py\n",
    "\n",
    "# Call the function to set up the project path\n",
    "setup_project_path()\n",
    "\n",
    "# Now you can import your modules\n",
    "from utils import helper as h_\n",
    "import ch_02.code_ch_02 as f_ch2\n",
    "import ch_03.code_ch_03 as f_ch3\n",
    "import ch_05.code_ch_05 as f_ch5\n",
    "import code_ch_06 as f_ch6\n",
    "import ch_08.code_ch_08 as f_ch8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_parquet(\"../../data/IVE_kibot.parq\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load ../data/variables_ch2.pkl\n",
    "%run ../ch_02/code_ch_02.py\n",
    "\n",
    "path = '../../data/variables_ch2.pkl'\n",
    "import pickle\n",
    "with open(path, 'rb') as f:\n",
    "    bars = pickle.load(f)\n",
    "    bar_time = pickle.load(f)\n",
    "    \n",
    "# df as bars['Dollar'].df_OLHC without 'cusum' column\n",
    "df = bars['Dollar'].df_OLHC.drop(columns=['cusum'])\n",
    "# For the purpose of this example, remove rows where time_close is duplicated\n",
    "# (keep the first row)\n",
    "# Remove rows where time_close is duplicated, keeping the first occurrence\n",
    "df = df.drop_duplicates(subset='time_close', keep='first')\n",
    "# set index as 'time_close'\n",
    "df = df.set_index('time_close')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CLOSE PRICE AND DAILY VOLATILITY\n",
    "# Step 1 : get the daily volatility\n",
    "close = df['close']\n",
    "dailyVol = f_ch3.getDailyVol(close, span0=100).dropna()\n",
    "\n",
    "# from series to df\n",
    "close = pd.DataFrame(close)\n",
    "dailyVol = pd.DataFrame(dailyVol)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6.3.3 Observation Redundancy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Understanding Out-of-Bag (OOB) Accuracy in Bagging:\n",
    "\n",
    "**Bagging (Bootstrap Aggregating):**\n",
    "- Bagging is an ensemble learning technique that involves training multiple models on different subsets of the training data and then aggregating their predictions.\n",
    "- In bagging, each model is trained on a bootstrap sample, which is created by randomly sampling the original training set **with replacement**. This means that some observations will be repeated in the bootstrap sample, while others might be left out.\n",
    "\n",
    "**Out-of-Bag (OOB) Observations:**\n",
    "- For each bootstrap sample, the observations that were **not selected** are called \"out-of-bag\" (OOB) observations.\n",
    "- On average, about 63% of the original training data is included in each bootstrap sample, meaning that around 37% of the data is left out as OOB observations.\n",
    "\n",
    "**Out-of-Bag (OOB) Accuracy:**\n",
    "- After a model is trained on the bootstrap sample, its performance can be evaluated on the OOB observations. This evaluation is done for each model in the ensemble.\n",
    "- The OOB accuracy is computed by aggregating the predictions of all the models for their respective OOB observations and comparing them to the true labels.\n",
    "- **Key Point:** OOB accuracy provides an estimate of the model's performance without the need for a separate validation set or cross-validation.\n",
    "\n",
    "### Understanding Cross-Validation (CV) Accuracy:\n",
    "\n",
    "**Cross-Validation (CV):**\n",
    "- Cross-validation is a technique used to assess the performance of a model by partitioning the data into multiple subsets or \"folds.\"\n",
    "- In **k-fold cross-validation**, the data is divided into `k` folds. The model is trained on `k-1` folds and tested on the remaining fold. This process is repeated `k` times, with each fold serving as the test set exactly once.\n",
    "- The final performance metric is typically the average accuracy (or another metric) across all `k` folds.\n",
    "\n",
    "### Key Differences:\n",
    "\n",
    "1. **Source of Data:**\n",
    "   - **OOB Accuracy:** Evaluated on the observations that were **not included** in the bootstrap samples (about 37% of the data per model).\n",
    "   - **CV Accuracy:** Evaluated on a specific fold in the data that was **held out** as the test set during cross-validation.\n",
    "\n",
    "2. **Data Overlap:**\n",
    "   - **OOB Accuracy:** Some OOB observations in different models might be very similar because the bootstrap samples are drawn with replacement. This can lead to inflated accuracy if the models have seen very similar observations during training.\n",
    "   - **CV Accuracy:** The test fold in cross-validation is explicitly kept separate from the training data, providing a more robust estimate of model performance.\n",
    "\n",
    "3. **Purpose:**\n",
    "   - **OOB Accuracy:** Provides an estimate of model performance directly from the training process in bagging without needing to set aside a separate validation set.\n",
    "   - **CV Accuracy:** Provides a more robust estimate of model performance by ensuring the test data is independent of the training data.\n",
    "\n",
    "4. **Potential Issues in Bagging:**\n",
    "   - The passage highlights that **OOB accuracy can be misleading** in the presence of observation redundancy, where the OOB observations are not truly independent from the training data due to the nature of the bootstrap sampling. This can lead to an overestimation of the model's true performance.\n",
    "   - **Cross-validation without shuffling** is recommended in such cases to get a more accurate assessment of model performance.\n",
    "\n",
    "### Summary:\n",
    "- **OOB Accuracy** is a convenient estimate of model performance in bagging but can be biased if the training and OOB sets are not truly independent.\n",
    "- **Cross-Validation Accuracy** is a more reliable measure of model performance, especially when there is a risk of data redundancy or dependence between training and testing samples."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The snippet you provided demonstrates three different approaches to setting up a Random Forest (RF) model, each with its own characteristics. Here's a breakdown of each setting and an explanation of the `bootstrap=False` parameter.\n",
    "\n",
    "### 1. **`clf0`: Standard Random Forest**\n",
    "```python\n",
    "clf0 = RandomForestClassifier(\n",
    "    n_estimators=1000,\n",
    "    class_weight='balanced_subsample',\n",
    "    criterion='entropy'\n",
    ")\n",
    "```\n",
    "- **`n_estimators=1000`**: The model consists of 1,000 trees.\n",
    "- **`class_weight='balanced_subsample'`**: This balances the classes in each bootstrap sample. For imbalanced datasets, it adjusts the weights inversely proportional to class frequencies in the input data.\n",
    "- **`criterion='entropy'`**: The model uses the entropy (information gain) criterion to split nodes.\n",
    "- **`bootstrap=True`**: Implicit because this is the default. The model uses bootstrap samples (sampling with replacement) to build each tree.\n",
    "\n",
    "This is the standard implementation of a Random Forest with 1,000 trees, using bootstrapping to create the datasets for each tree.\n",
    "\n",
    "### 2. **`clf1`: Decision Tree with Bagging**\n",
    "```python\n",
    "clf1 = DecisionTreeClassifier(\n",
    "    criterion='entropy',\n",
    "    max_features='auto',\n",
    "    class_weight='balanced'\n",
    ")\n",
    "clf1 = BaggingClassifier(\n",
    "    base_estimator=clf1,\n",
    "    n_estimators=1000,\n",
    "    max_samples=avgU\n",
    ")\n",
    "```\n",
    "- **`DecisionTreeClassifier`**: A single decision tree is used as the base model.\n",
    "  - **`criterion='entropy'`**: Similar to `clf0`, using entropy for node splitting.\n",
    "  - **`max_features='auto'`**: The number of features considered for splitting is automatically chosen.\n",
    "  - **`class_weight='balanced'`**: Balances class weights based on class frequencies.\n",
    "- **`BaggingClassifier`**: This creates an ensemble of decision trees using bagging.\n",
    "  - **`n_estimators=1000`**: 1,000 decision trees are used.\n",
    "  - **`max_samples=avgU`**: This limits the size of each bootstrap sample to a fraction of the original dataset, determined by `avgU`.\n",
    "\n",
    "This approach uses Bagging to create an ensemble of decision trees, which is a more flexible form of RF, where you can control the sampling process explicitly.\n",
    "\n",
    "### 3. **`clf2`: Bagging with a Single Tree per Bootstrap Sample**\n",
    "```python\n",
    "clf2 = RandomForestClassifier(\n",
    "    n_estimators=1,\n",
    "    criterion='entropy',\n",
    "    bootstrap=False,\n",
    "    class_weight='balanced_subsample'\n",
    ")\n",
    "clf2 = BaggingClassifier(\n",
    "    base_estimator=clf2,\n",
    "    n_estimators=1000,\n",
    "    max_samples=avgU,\n",
    "    max_features=1.\n",
    ")\n",
    "```\n",
    "- **`RandomForestClassifier`**: This is configured to build only a single tree.\n",
    "  - **`n_estimators=1`**: Only one tree is built.\n",
    "  - **`bootstrap=False`**: **This is the key difference**. No bootstrapping is done; the tree is built on the entire dataset, which means it will be a fully deterministic tree.\n",
    "- **`BaggingClassifier`**: Creates an ensemble of 1,000 models.\n",
    "  - **`max_samples=avgU`**: Uses a fraction of the data for each tree, determined by `avgU`.\n",
    "  - **`max_features=1.`**: Only one feature is considered for splitting at each node, adding further randomness to the model.\n",
    "\n",
    "This setup is an unusual use of a Random Forest where each tree is built on a non-bootstrapped dataset. The ensemble is created using Bagging, which adds randomness by subsampling the dataset and limiting the features considered at each split.\n",
    "\n",
    "### **`bootstrap=False` Explained:**\n",
    "- In a standard Random Forest (`bootstrap=True`), each tree is built on a bootstrapped dataset (a sample with replacement from the original dataset). This introduces variability into the trees, even when trained on the same data, because each tree sees a different subset of the data.\n",
    "- **`bootstrap=False`**: When set to `False`, each tree is built on the entire dataset without sampling. This can lead to less diversity among the trees in the ensemble because they are all trained on the same data.\n",
    "\n",
    "### **Distinctions Between the Three Models:**\n",
    "1. **`clf0`**: Standard Random Forest with bootstrapping, using all available data for each tree. This is the most commonly used setup.\n",
    "2. **`clf1`**: An ensemble of decision trees created through bagging, allowing for more control over the sampling process (e.g., limiting sample size).\n",
    "3. **`clf2`**: A Random Forest with `bootstrap=False`, meaning each tree is built on the entire dataset without resampling, combined with Bagging to introduce some variability through subsampling and feature selection.\n",
    "\n",
    "In summary, `clf2` deviates the most from standard Random Forests by avoiding bootstrapping and instead relying on Bagging to generate variability. This setup can be particularly useful in scenarios where bootstrapping is not desirable or effective."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7.5 Bugs In SKLEARN Cross-Validation\n",
    "\n",
    "Regarding the bugs mentioned in your book about scikit-learn's cross-validation, here is an update:\n",
    "\n",
    "1. **Scoring functions not recognizing `classes_` (Issue #6231):** This issue was related to scikit-learn's scoring functions not recognizing the classes when working with certain classifiers, leading to incorrect scoring when using cross-validation. This issue has been acknowledged and discussed within the scikit-learn community, and a solution was implemented. The scorer objects now look at the `classes_` attribute of the estimator before cross-validation, addressing the problem.\n",
    "\n",
    "2. **`cross_val_score` issue with `log_loss` (Issue #9144):** The problem was that `cross_val_score` did not allow the labels to be passed to the `log_loss` function, which could result in errors when the training and testing sets contained different classes. This issue was also addressed by improving how scikit-learn handles labels within its scoring functions.\n",
    "\n",
    "Both of these issues have been addressed in newer versions of scikit-learn. If you are using a recent version of the library, these bugs should no longer affect your work. However, it's always a good practice to review the documentation and release notes of the libraries you're using to stay informed about any ongoing or resolved issues【683†source】【684†source】."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "financial_math",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
